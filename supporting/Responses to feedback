R1: The authors present a study evaluating a number of XAI approaches for digital pathology in healthcare. Overall, I am very positive to the paper. It targets an underserved but crucial research area, and does so with a well-crafted study with a solid discussion of the results leading to important insights for future efforts.
%	Thank you very much for this positive comment. The parts of the manuscript detailing the crafting and results of the study remain mostly unchanged, barring the minor revisions suggested. In light of valuable points for major revision, the discussion of these results and the conclusions derived thereof have been shifted to focus on those findings that can be reliably generalised away from the specific implementations chosen. We believe that this addresses many of the limitations identified in the first submission, and makes for a far stronger contribution to Special Issue and the literature overall.

R2: In this article authors demonstrated the need of explainable models in pathology domain, specifically in digital pathology applications. Digital pathology is gaining a lot of attraction recently and pathologists are beginning to get used to the digital tools that are provided. However, as the authors mentioned the acceptance from the pathologists is questioned most of the time. Overall, authors captured the problems in applying machine learning tools to digitized pathology images and provided possible solutions to overcome these problems. Article is well written and has merit (although lacks novelty as most of the issues are already been discussed in the literature)
%	We are very grateful for the positive comments of R2 and can assure our reviewer that - to the best of our knowledge - our work represents the first practical evaluation in this domain and thus the novelty factor is absolutely guaranteed. As the reviewer correctly states, many issues have already been discussed - but as yet, only a theoretical perspective.

1.1.  The highlights should be given a bit more nuance. While the full story is given in the main text, it is likely that the formulation of the highlights will be what is spread most from this work, so they need to be precise:
1.1.1 The first bullet appears a bit bold. I don't know of a dedicated XAI study with similar form to this one, but since there are XAI aspects in other work in digital pathology evaluations with pathologists, the statement of being "the first study" should be given more nuance.%	-
1.1.2 The negative statement of salicency maps and prototypes is too far-reaching in my view. I agree that the results show that for this application and for the design choces made, those options are not good. But the results (as is discussed) does not rule them out for other situations. Besides, I consider trust scores also to have a high risk of leading to false trust (more about that below). The statement needs to be made more nuanced.%	-
1.1.3 In the highlights, I miss having something from the generic discussion, not being tied to a specific XAI approach. I think one or two main conclusions are relevant to promote to this list.
%	We agree with the assessment of R1 that the highlights, and the paper findings they summaried, lacked nuance, made sweeping statements about individual methods that were difficult to generalise beyond the immediate scope of this study. These highlights have been updated to reflect the overarching interpretations and implications that we were able to draw from the study, representing a more nuanced and impactful set of findings.

1.5 The paragraph with mathematical description of saliency maps reads as too high level of detail for this paper, especially compared to how other methods are described. Most of it could probably be removed. (And I was confused by the 'c' variable, is it used for two different things?)
2.1. Related work section is extensive but looses focus of the paper by hopping between subtopic. I recommend authors to shorten the related work, only keeping the direct relative work with their proposed approach. On the other hand, it looks incomplete when giving definitions of Saliency Maps (SM)  and Concept attribution (CA), but not even defining Counterfactuals (CF), Prototypes (PR),  and Trust scores (TS). Authors have to find the balance. Another example is the last paragraph of related work, which carries a topic out of nowhere related to the rest of the section.
%	Thank for these valuable pieces of feedback on the introduction section. We agree with the assessment of the reviewers, and have updated this section for cohesion, and to provide a more appropriate level of detail when introducing the explainability methods, distributed evenly over the classes referenced. It also makes broader reference to the literature, in particular to relevant work in the HCI domain, in order to provide a comprehensive overview of the state of the art.

The resistance to new technology was handled in prior articles such as : - Tosun, Akif B., et al. "Explainable AI (xAI) for Anatomic Pathology." Adv Anat Pathol 27.4 (2020): 241-250. (this was not cited in the article, highly recommend authors to consider citing, already addresses some concerns mentioned in this manuscript (S4.4.10))
%	The authors are aware of this piece of prior art, and it has now been referenced in the related work, along with a more directed point on resistance to new technology. Unfortunately, for such a promising and relevant implementation of xAI in pathology, the paper does not reveal much detail about the HistoMapr interface itself. We are eagerly awaiting a follow-up publication from this group.

The main reason of this resistance is the complexity of the deep learning algorithms (for a pathologist to digest) and unclear, not-practical outcomes of the software (created trust issues). Moreover, deep learning methods are mostly vulnerable to adversarial attacks, and this creates further doubt among pathologists, which was handled in this article: Foote, A. et al. "Now You See It, Now You Dont: Adversarial Vulnerabilities in Computational Pathology." ArXiv abs/2106.08153 (2021): n. pag. (this was also not cited in the article, highly recommend authors to consider citing, already addresses some concerns mentioned in this manuscript (S5.3))
%	The topic of adversarial vulnerability is a very important one, and a note addressing this has been added in the introduction, along with a reference to this article.

1.6. Section 2, "The goal of all these efforts..." - I would say that trust is one goal, but not the only goal. For instance, increasing diagnostic performance would be an overarching one. Also, "trust is the only mechanism..." seems too categorical, "...a key mechanism..." is perhaps better.
%	This part of the introduction has been rewritten to provide a more conservative, literature-driven overview of the state of the art. The question of trust as a driving factor in xAI has been linked to the appropriate sources	

1.7. The description of how the concept attribution used in the study was generated should be expanded.% More detail of how the concept attribution, as well as the other explanation methods, were created and on which prior art they were based, has been added to the method. 

1.8. In 3.3 and 4.1, clarify the removal criteria for "extreme value" responses/participants.%	Upon reviewing the data, the extreme valued sample in question turned out to be a test submission from one of the authors. The data point and references to it were removed.

1.9. Figure 4: I would like to switch the likert scale to have "strongly agree" to the right. I may be wrong, but this orientation seems non-standard, and it would be good to be consistent with fig 3 as well.%	We agree with this and have switched the axes, this also aligns more closely with the presentation of this scale in the study,

1.10. In 4.4.10, the "brochure" proposal seems to be identical to what was proposed in this paper, perhaps cite it: Sendak, Mark P., et al. "Presenting machine learning model information to clinical end users with model facts labels." NPJ digital medicine 3.1 (2020): 1-4.%	Thank you for pointing out this relevant piece of literature. It is indeed almost identical, and we have include a point referring to this work to the Discussion/Implications section

1.2.  There is one limitation I think should be more pronounced. Since the end result of a Ki-67 review is an region-of-interest aggregate percentage relating to a cut-off, the scrutiny of individual cell nuclei (which the XAI approaches focus on) would often be overkill in a clinical setting. While I still agree that for this study Ki-67 is a good application choice for many reasons, this limitation is important to understand - for instance when interpreting the responses to question 4 in the survey.%	We agree that this is a limitation of the study with regards to the findings we claimed in the first submission. The revisions to the manuscript now place less emphasis on evaluations of specific techniques, thus reducing the impact of this limitations. Nevertheless, we address this directly in a new paragraph in the limitations section.

1.3. I think the positive results for trust scores should be challenged a bit more in the discussion. The pathologists' underlying assumption when praising it is that the trust score itself can be trusted. Such a distinct concept as a seemingly exact number also gives the impression of being trustworthy. However, the trust score for a case that is out of distribution wrt to the training set is not trustworthy, i.e. potentially misleading. There is a risk here that is enhanced by the fact that the score is seemingly easy to grasp.
%	 We have taken this valuable piece of feedback and integrated it as a primary example of the challenges of integrating explainability into AI systems. This danger of appealingly digestible explanations is one of the core implications for our research.

1.4. Another discussion item that could be given more weight is the tendency to give high scores for approaches "doing the same that I do". This is natural, but you could argue that a more effective angle would be that XAI approaches should be "orthogonal" to what the human experts do/are good at. This mindset could potentially also address the antropomorphism challenge.
%	Thank you for this insightful piece of feedback. We have integrated this 'orthogonal' suggestion into the main suggestions for future work, as it does indeed represent a sound strategy for addressing the biases identified.

1.11. In 5.3, an unfinished sentence at the end of the seocnd paragraph.
%	This error has been corrected.

2.3. The negative side of showing explanations is not tested thoroughly in the study. In the questionnaire, authors should consider asking the participants, if the given explanations were too much (e.g., overwhelmingly too much information to parse) and it was time consuming to go through, etc.
%	Without a focus on the usability of specific methods, this limitation plays a much smaller role. We have nevertheless made reference to it in the limitations identified for the study.

2.2. The experiment was conducted on a very odd scenario. the task of detecting positive cells should not need an explanation, however diagnosing an entire tissue section with a label should need an explanation. I would recommend authors to repeat the experiment with a more complex task, such as grading a prostate tissue sample, or staging a colon tissue sample, or diagnosing a breast duct.
%	Thank you for this accurate assessment of the limtations of implications of the study, as previously interpreted. We have now reframed the discussion of the results to focus more heavily on overarching implications of the study, such that these are less sensitive to the specific implementation. We are reassured by the comments of R1 that the Ki-67 use case is an appropriate target in and of itself, and have developed the method section to better justify this choice. The topic of applying explainability in inappropriate use cases is also explicitly addressed as one of the implications of our research, and the suggestion for a study into a more complex task integrated into the future work suggested.