
\subsection{Limitations}
\label{sec:Limitations}
With regard to limitations of the survey and interviews, almost all interview participants mentioned the difference of the presented survey and images to a realistic diagnostic workflow. The survey lacked the ability to see different regions, show the original image without annotations, the H/E layers, and generally look around the slide.
Furthermore, the lack of annotations on some nuclei hindered the interpretation of the results, since it was not clear whether there were only cancer cells marked, if positivity was only judged based on the annotated cells, and XY. Generally, this should really be a two-step process, where first the cancer cells are identified and the the Ki-67 positivity is computed.

\subsection{Use of AI in Pathology}

%use AI as extension of simple tools, saving time as uppermost goal
%also use AI as flag, impartiality as benefit -second set of eyes
%example of Ki67, good for tedious tasks requiring accuracy 
%could use parameters from experienced pathologist for unexperienced ones

\subsection{Limitations of the use of AI in Pathology}

%slide digitalization takes a long time -> reach critical point for it to become feasible
%data protection laws vs cloud based solutions -> provide opportunity for local storage!
%lack of mutual understanding
%information about quality of training data
%Ki67 task, lack of standardized staining
%generally, interactivity opposes idea of standardization

\subsection{Wishes, Suggestions, Requirements}

%trust
%subject AI to different forms of validation
%diversity and experience of annotating pathologists - min 3 diff path
%require reliability to hand over control to AI

%
%prefer simple, visual explanations -> similar to how they think and learn
%AI makes decision in relatable manner (thinking the same way)
%interactivity

\subsection{Pitfalls}
%classes easy to misunderstand

\subsection{Future Ideas}
A new approach that was explored during an interview is the use of real images from the training data as prototypes for selected annotations. That is, upon selecting a classification, the xAI solution shows a comparable image selected from the training data. By doing this, it does not pretend to understand and explain complex, interconnected topics, but rather relies on the Pathologists expert knowledge to assess the quality of the classification. Most importantly, it does not reduce the dimensionality of the image to certain features.
This approach also takes the similarity of modality into account by presenting a visual representation, and furthermore aligns itself with
the conventional Pathologist workflow of looking up examples in reference books.


% Another key aspect that increases trust is if the AI solution is perceived as ``thinking" the same way as the pathologist (P1, P3). That is, if it struggles with the same problems that a Pathologist struggles with, or looks at the same areas an expert would take into consideration (P1), or just provides examples that look similar to the prototypical characteristics a Pathologist would expect (P3). This can be especially important for techniques such as Prototypes and Saliency Maps that can easily be (mis)interpreted as thinking the same way as the users. Three participants (P1, P3, P5) remarked that Pathologists are most often thinking in visual terms, so that catering to this modality is a major way of mirroring the Pathologists way of thinking and gaining trust.

% Since the Pathologist is ultimately always the single responsible person for a diagnosis, allowing an AI solution to take control and for example set thresholds requires a great deal of trust. When inquired what would it take to allow this, a namely answer was a good AI solution that does these tasks correctly (P4). Checking the tasks manually is also always considered (P4, P6), as well as additional methods of verification (P2).
