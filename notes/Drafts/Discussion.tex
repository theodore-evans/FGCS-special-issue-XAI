
\subsection{Limitations}
\label{sec:Limitations}
With regard to limitations of the survey and interviews, almost all interview participants mentioned the difference of the presented survey and images to a realistic diagnostic workflow. The survey lacked the ability to see different regions, show the original image without annotations, the H/E layers, and generally look around the slide.
Furthermore, the lack of annotations on some nuclei hindered the interpretation of the results, since it was not clear whether there were only cancer cells marked, if positivity was only judged based on the annotated cells, and XY. Generally, this should really be a two-step process, where first the cancer cells are identified and the the Ki-67 positivity is computed.

\subsection{Use of AI in Pathology}

%use AI as extension of simple tools, saving time as uppermost goal
%also use AI as flag, impartiality as benefit -second set of eyes
%example of Ki67, good for tedious tasks requiring accuracy 
%could use parameters from experienced pathologist for unexperienced ones

\subsection{Limitations of the use of AI in Pathology}

%slide digitalization takes a long time -> reach critical point for it to become feasible
%data protection laws vs cloud based solutions -> provide opportunity for local storage!
%lack of mutual understanding
%information about quality of training data
%Ki67 task, lack of standardized staining
%generally, interactivity opposes idea of standardization

\subsection{Wishes, Suggestions, Requirements}

%trust
%subject AI to different forms of validation
%diversity and experience of annotating pathologists - min 3 diff path
%require reliability to hand over control to AI

%
%prefer simple, visual explanations -> similar to how they think and learn
%AI makes decision in relatable manner (thinking the same way)
%interactivity

\subsection{Pitfalls}
%classes easy to misunderstand

\subsection{Future Ideas}
A new approach that was explored during an interview is the use of real images from the training data as prototypes for selected annotations. That is, upon selecting a classification, the xAI solution shows a comparable image selected from the training data. By doing this, it does not pretend to understand and explain complex, interconnected topics, but rather relies on the Pathologists expert knowledge to assess the quality of the classification. Most importantly, it does not reduce the dimensionality of the image to certain features.
This approach also takes the similarity of modality into account by presenting a visual representation, and furthermore aligns itself with
the conventional Pathologist workflow of looking up examples in reference books.


% Another key aspect that increases trust is if the AI solution is perceived as ``thinking" the same way as the pathologist (P1, P3). That is, if it struggles with the same problems that a Pathologist struggles with, or looks at the same areas an expert would take into consideration (P1), or just provides examples that look similar to the prototypical characteristics a Pathologist would expect (P3). This can be especially important for techniques such as Prototypes and Saliency Maps that can easily be (mis)interpreted as thinking the same way as the users. Three participants (P1, P3, P5) remarked that Pathologists are most often thinking in visual terms, so that catering to this modality is a major way of mirroring the Pathologists way of thinking and gaining trust.

% Since the Pathologist is ultimately always the single responsible person for a diagnosis, allowing an AI solution to take control and for example set thresholds requires a great deal of trust. When inquired what would it take to allow this, a namely answer was a good AI solution that does these tasks correctly (P4). Checking the tasks manually is also always considered (P4, P6), as well as additional methods of verification (P2).



% \subsubsection{Trust Scores} 

% The least controversial of all of the examples presented, with a general consensus that some measure of confidence is expected from an AI solution, in order to help the pathologist understand the limitations of the output presented and direct their attention to results that may need reviewing. The general acceptance of this type of explanation may also be attributed to its perceived utility as an interface for actively reviewing and refining the performance of an AI solution.

% Although no causal factors were intended to be conveyed by the reported annotation confidences presented, there was a tendency for these to nonetheless be interpreted by the user. We interpret this as a second-order effect, whereby the AI solution is rendered more relatable through its fallibility or simply by merit of the fact that it seems to share the same challenges as the pathologist in reaching its conclusion. While presenting an opportunity for positive trust-building, this approach should be treated with due care, as the implied comparability of decision-making processes can not be consistently relied upon.

% While there exists the possibility that participants were guided toward such an interpretation based on our presentation of these confidences \textit{as} explanations, we suggest that this result highlights a general proclivity to anthropomorphize the AI solution. 

% \subsubsection{Counterfactuals} 

% These were generally well-received, particularly in the interview setting. The example presenting a single-axis of gradually changing annotations, with a clear decision boundary delineating one class from another, was identified as being particularly apt for the task at hand. The positive response to this example can be viewed in this light, as well as indicating a general suitability of this type of explanation to the visual decision-making processes common in pathology.

% The divisive response to the two-axis implementation of counterfactuals highlights challenges in their effective usage. On the one hand, the more complex implementation was variously considered more confusing and/or distracting than its simpler counterpart. On the other, it was identified that even in this more complex presentation it did not allow the user to disentangle the different factors changing between classifications, in order to determine which were more critical to the solution's outcome. For the xAI designer, the decision of whether to err on the side of presenting more or less information is a non-trivial one.

% \subsubsection{Concept Attribution} 

% The examples presented were met with a mixed reception. While performing well in the online questionnaire, likely due to its explicit presentation of causal factors, it received criticism for its textual format and lack of precision.

% In general, we suggest that text-based explanations levy an additional cognitive load on the pathologist, due to the imprecise abstraction of the rich visual information in their perceptive field during diagnostic work. This appears to place any text-based implementation of this explanation, in an untenable position. Adding detail to address the shortcomings in precision further reduce its viability, while re-interpreting the explanation in visual form tends to bring it outside the realm of what might reasonably be described as `concept attribution.' Instead, pathologists' re-interpretations of such explanations in visual format tended more toward descriptions of counterfactual-type implementations than any visual variants of concept attribution (e.g.,\ feature visualisation, as applied to a digital pathology context by \citet{stacke_measuring_2021}).

% We cannot rule out the possibility of these suggestions as being guided by our presentation of counterfactuals, and acknowledge the possibility of promising approaches involving visual representations of decision-relevant concept. However, based on current research, we speculate that the explanations produced by such methods will prove too abstract to be valuable to pathologists in a clinical setting.

% Despite the limitations identified, the research suggests promising elements to this class of explanation. For one, the straightforwardness with which important factors are presented effectively meets the pathologist's need for a simple explanation, maybe accounting for the positive response to these examples despite the limitations mentioned above. Moreover, several promising applications for the automated generation of structured annotations of whole slide image data were implied, particularly with respect to ongoing standardisation efforts~\cite{daicom:supplement:222}. We suggested that with respect to such applications, textual concept attribution may be an important xAI capability.

% \subsubsection{Prototypes}

% Identified as both the most intuitively understandable, yet at the same time least informative, explanation presented. By their nature, prototypes tend to represent the least controversial examples of a given class or type. This may be valuable in some contexts -- for instance, to identify issues with slide quality or to determine that an AI solution is correctly identifying features to resolve between very similar classes.

% However, the attractive simplicity of this type of explanation, coupled with its low information content, was identified as a potentially misleading combination. By presenting non-controversial examples as justification of an overall result, the pathologist risks being lulled into over-confidence in the AI solution, particularly when these examples fit into their own mental schema of archetypal examples.
    
% Improvements suggested by pathologists that aim to address these issues. For example, presentation of a large variety of examples, including or even weighted toward those that fall close to the decision boundary of the solution -- tended to describe explanations falling outside the scope of those reasonably considered as employing prototypical examples, and more toward other classes such as Trust Scores and Counterfactuals. While it can not be ruled out that these re-imaginings of this type of explanations were primed by the other examples presented, it was indicated that by their very nature, prototype-based explanations fail to capture the diversity inherent to the subject matter of diagnostic pathology.

% \subsubsection{Saliency Maps}

% These were the least well-accepted form of explanation, criticized for their ambiguity, lack of clarity and potential to mislead. The fundamental concept of a saliency map is to explain an output based on the identification of important regions of an image. As indicated by pathologists in our findings, only limited information about diagnostically relevant factors can be communicated simply by showing the spatial distribution of important features. 

% This can be understood in1.2.  There is one limitation I think should be more pronounced. Since the end result of a Ki-67 review is an region-of-interest aggregate percentage relating to a cut-off, the scrutiny of individual cell nuclei (which the XAI approaches focus on) would often be overkill in a clinical setting. While I still agree that for this study Ki-67 is a good application choice for many reasons, this limitation is important to understand - for instance when interpreting the responses to question 4 in the survey. terms of the large number of variational factors that can change for any given region of the slide. In highlighting a given region as particularly salient to some output, there exist a wide variety of interpretations about which of these factors were important or would need to change for the outcome to be different. Moreover, this degeneracy leaves open the possibility for highlighting salient regions to simply confirm a users' expectations regarding relevant features, even though these may significantly deviate from those taken into account by the AI solution. In this way, saliency maps carry the risk of uninformative misleading.

% It should be noted that the information provided by saliency maps appears poorly suited to the chosen grading task -- i.e.\ where the AI output already constitutes one or more segmentation masks, each indicating the important regions of an image concerning one or more target class. In such cases, the information conveyed by saliency maps is liable to be redundant and therefore uninformative and/or confusing. A suggested context to which saliency maps may be suitable is in explaining classification tasks, where the spatial distribution of relevant features may be more informative \cite{LapuschkinEtAl:2016:LRP}. 

% While we encourage the poor reception of these examples in our findings to be interpreted in this light, we suggest that the pitfalls described here may apply more generally.

% \subsection{Integration}

% To date, the vast majority of research regarding xAI approaches for pathology has focused heavily on the development of algorithmic approaches, rather than on the requirements and experiences of pathologists \cite{tjoa_survey_2020, poceviciute_survey_2020, antoniadi2021current}. 

% While xAI research is widely hailed as addressing the inherent risks of black-box processes inherent to AI solutions for pathology, little work has focused on the potential risks introduced by inappropriate or inadequately tested explanations for AI solutions. This research is novel in highlighting the potential risks associated with xAI implementations, which we propose to be a critical 

% While the need for user studies in xAI has been highlighted in the recent literature \cite{tjoa_survey_2020, antoniadi2021current}, only a small number of such studies have actually been carried out. So far, these have relied solely on the speculation of pathologists on desirable forms of explanation, based on some sample AI output \cite{liao2020questioning, cai2019hello}. While we have no doubts about the value of such work, we firmly believe that an accurate representation of the usability and, in particular, unintended consequences of xAI approaches, can only be observed through the interaction of pathologists with real or simulated xAI interfaces.

% Many of the pitfalls of explainable interfaces observed in our findings can be attributed to the tendency to anthropomorphize the AI solution. Recent work showing the vulnerability of even the most advanced deep-learning-based image analysis techniques to adversarial attacks provides a striking demonstration that neural networks do not understand images the same way humans do~\cite{geirhos2020shortcut}. \cite{foote2021now} show further evidence for the susceptibility of deep-learning based methods in Pathology by showing that a model classification can be flipped with imperceptible perturbations and by generating a single perturbation matrix which consistently fools a network on unseen test images. They also highlight that explanatory techniques like saliency maps themselves are not enough to ensure robust model features, and recommend the inclusion of adversarial attacks for performance evaluation of those models.
% The expectation of AI system to employ a human-like reasoning process, or moreover, to produce a human-like explanation creates a dangerous disconnect between expectations and reality of AI capabilities -- with implications for the importance and content of future AI/ML training curricula for pathologists.

% Some of the findings are reflective of well-established principles of UI/UX design. For instance, the lower cognitive load imposed by interface elements analogous to familiar processes \cite{cao2009modality}. As well as underlining the importance of this principle in the context of AI for pathology, this research begins to identify those processes and mental schema familiar to the pathologist, informing effective and convincing implementations for xAI in this domain.

% \subsection{Implications}%. what does it mean for the (new) state of the art?

% The diagnostic process in pathology relies heavily on the visual processing of cell morphology. Pathologists undergo years of training and practical experience, often with extensive sub-specialization, to build up expertise in the interpretation of histologic findings. This visual reasoning can, and often must, be expressed in natural language (written reports), for the purpose of communicating the findings to colleagues and patients. However, this translation can be challenging and imprecise - by necessity, it is a reduction of information that does not capture the rich biological complexity of the specimen. Where an explanation is required, integrative explanations are preferred; collecting and highlighting important visual features, contextualising these where necessary with simple descriptions in clear, concise language. We suggest that the perceived usability and value of AI explanations to pathologists is strongly tied to how closely they map to this conceptual space of familiar explanatory processes.

% However, given the inherent challenge of explaining complex decision-making processes in a suitably concise format, it is fundamentally unavoidable that much of this complexity not be represented. This burdens any potential explanation modality with the risk of inspiring over-confidence through an \textit{appeal to ignorance} -- the implication that that, because some factor is not represented in the explanation, it did not play a role in the decision-making process. This has the dangerous potential for obscuring inappropriate or confounding factors in the AI solution's decision-making logic while bolstering trust in this potentially flawed result.

% Moreover, the openness to interpretations for many xAI approaches brings the danger of building unsubstantiated trust in AI solutions through positive confirmation bias. Particularly under time pressure, the tendency to find confirmations of existing expectations in ambiguous data is well known. While some types of explanation appear particularly susceptible to such fallacious interpretations (saliency maps and prototypes being the stand-out examples from our findings), we understand it to be an inherent risk associated with any form of explanation presented to the user.

% Far from being a criticism of pathologists for their susceptibility to such logical pitfalls, we interpret these as expressions of the quintessentially human tendency to anthropomorphize AI solutions in order to make sense of their actions. This places upon developers and designers of xAI for pathology a responsibility to preempt and mitigate such misinterpretations. As such, our findings strongly advocate for a cautious approach to this process, with proposed explanations subject to continual validation and feedback from a live target audience. 

% This strongly underlines the need for close collaboration between AI/ML experts, UI/UX designers and pathologists in developing xAI solutions for this domain. We believe that this entails both a transdisciplinary strategy for xAI design and an active approach to exchanging domain expertise between these stakeholders. It is the opinion of the authors that many of the risks identified in this research can be mitigated by a holistic understanding of the diagnostic workflow by xAI designers, coupled with a well-informed expectation of AI capabilities by its users. 

% Also implied is the existence of promising and as-yet unexplored approaches to xAI for pathology, in which the needs and proclivities of their users are integrated into the design process from the outset, made possible by closer cross-disciplinary collaboration. Some suggestions for future work arising from our research are proposed in section~\ref{sec:FutureWork}.
  

% \section{Conclusion}
% \label{sec:Conclusion}

% In this work, a representative set of xAI implementations for pathology were prepared and evaluated, for the first time, by a cohort of clinical pathologists. These findings are represented in the aggregated responses of 25 online questionnaire responses combined with the analysis of six hours of expert video interviews. This research identifies good practices and novel opportunities for xAI solutions for pathology that inspire well-placed trust from their users while highlighting substantial risks to which developers and UI/UX designers must be attentive.

% To this date, there have been no xAI studies in the domain of Digital Pathology which put a focus on the end-user. Most studies come from a technical standpoint,
% and evaluate the usability of singular AI approaches - mostly as pure literature review such as \cite{yang2021unbox} and \cite{poceviciute_survey_2020}, with first steps also being made in the direction of exploring the needs of medical practitioners and resulting design patterns~\cite{liao2020questioning,cai2019hello,wang_designing_2019}. The novelty of our approach is in the qualitative evaluation of different approaches from different experts, crafting a comprehensive and more representative overview of the reception of different xAI approaches. Basing those insights on the discussion with domain experts in invaluable for a faithful evaluation.

% Our findings demonstrate the preference of pathologists for simple visual explanations that mirror their way of thinking and integrate cleanly with their diagnostic workflow. They also suggest dangers associated with explanations that are overly appealing in their simplicity, or allow for too much ambiguity in their interpretation.

% The quality of an explanation is also tied to its suitability for a specific diagnostic task, as a function of its alignment with the decision-making processes considered important and familiar. Therefore, any assessment of relative usability should take this into account. Nevertheless, it is possible to make some general statements about the various forms of explanation proposed in the literature on explainability for pathology.

% Regarding these individual methods, we find \textbf{counterfactual}-based approaches the most suitable for this type of diagnostic task, with a strong indication for their general applicability for xAI solutions in pathology. \textbf{Trust scores} were indicated to be an acceptable, and often expected, accompaniment to AI output. They should, however, be subject to the same scrutiny as any other explanation with regard to their capacity to mislead or instill over-confidence.

% We suggest that in many diagnostic contexts, \textbf{saliency maps} may be at best redundant, and at worst confusing and/or misleading explanation modalities. While some more appropriate applications are put forward, it is strongly recommended that this modality be applied with caution. Likewise, \textbf{prototype}-based explanations, where overall results are supported by the prime (and therefore likely least controversial) examples of classes or types, carry the dangerous risk of instilling unsubstantiated trust in results, particularly in light of their apparent simplicity. While interesting potential applications for \textbf{concept attribution} are suggested, text-based implementations are indicated to be largely unsuitable for routine diagnostic work.

% Above all else, this paper highlights the importance of close collaboration between ML experts, AI developers, UI/UX designers and end-users for the safe and effective development of trustworthy xAI applications for digital pathology.

\section{Future work}
\label{sec:FutureWork}

We suggest that the safest and most effective forms of explanation for AI solutions in pathology may be those that strike the balance between relatability to the pathologist and honesty in representing the decision-making processes and capabilities of the AI system.An example that stands out from those suggested by pathologists in this study, is that of an explanation-generating approach that represents the distribution and examples of the samples present in the training data that were most influencial to a given result. This is both familiar to the pathologist, an analogue to the process of referring to past cases and reference material in justifying a decision, and remains true to the statistical methods underpinning the operation of neural networks.

% Motorwarnleuchte

% A separate but related approach is in helping the pathologist make the best-informed diagnostic decisions, particularly for the outputs of specific AI solutions, by supporting them with rich, multi-modal data intuitively integrated into the digital workspace. This could include the automatic generation and smart display of structured annotations on slide data, automatic application of supportive AI solutions for feature detection, cross-referencing and highlighting conflicts in diagnostically relevant AI results.

% It is above all the author's hope that this research serves as a good template for holistic, user-centric xAI research in pathology and further afield. There are many opportunities to validate and build upon these results, accounting for the limitations of this study. We recommend future studies to evaluate the responses of pathologists to real implementations of some of the explainability examples presented and/or suggested here in a realistic, interactive interface.