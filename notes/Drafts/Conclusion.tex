

% \section {Conclusion}

% -trust on training data -> pitfalls, non-realness of explanation
% -sm pitfalls -> implement sm responsibly
% -desire for interactivity -> implement this, but what about standardization
% -open and new explanation types -> focus on collaboration
% -limitation with local/cloud based -> vendors should allow local solutions
% -proposed new mechanism of using training data images


% It is important to keep in mind that explanations are most often suited to certain tasks, which hinders an overall comparison of explainability. This affords further research with other image explanation tasks to explore new facets of xAI classes. 


% We find providing comprehensive information on the training data for AI solutions as third important factor for building trust. Especially a variety of annotating Pathologists as well as information about the providing institution was required by our interviewees. 
% Regarding our second research question of the strengths, limitations and risks of different xAI classes, we find a strong possibility for building trust in AI solutions with the approaches of visualization, relatable decision-making processes and information about training data. This is however a double-edged sword. On the one hand, trust is required to fuel the adoption of AI solutions. But it is also accompanied by the risk of building trust in non-trustworthy AI solutions, which is especially critical in the healthcare sector.


%  The findings are valuable for further development of future digital pathology solutions (again, the key findings briefly, clearly, crisply presented and future work touched upon).
 
% % future work
% -investigate the difference between what the user understands and what actually goes on (create experiments and methods to measure this gap and measure it for the different methods). -> One example: GAN/Prototypes and how much of the decision boundary they really cover or the fact that they "invent/interpolate" images instead of showing real ones. Another take: Investigate, how far Prototypes can discover errors/misbehaviours in the models or on real o.o.d. data.
% - Extend the investigation on other staining methods (HE) and especially, other diagnosis TASKS (not just cell counting).

% the need / expectation for relatable explanations becomes problematic when observed from a conservative vantage point, with respect to the current capabilities of ML/DL. 



% On the one hand, this motivates better training for pathologists in ML/AI, so that they better understand that strengths and limitations of these systems

% On the other, it also informs types of explanations that honestly represent the decision-making `logic' employed by today's ML architectures. Statistical approaches, linking results with relevant samples from training data, as suggested, it one such approach. Not at the expense of being relatable (referring to reference books, cases).