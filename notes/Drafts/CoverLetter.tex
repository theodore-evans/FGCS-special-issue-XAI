Especially in Pathology, the decision process hidden behind the Black Box has to be unravelled to allow the Pathologist to understand the represented features and ultimately trust the AI solution. We present a mixed-methods study that explores the core factors driving the adoption of explainable AI in Digital Pathology, and discusses the risks and problems yet to overcome. For that, we combined an online survey with expert interviews, to extensively assess the user understanding of five major xAI classes and general medical AI applications.
Since xAI approaches are relatively new in the young field of Digital Pathology, this survey is to our knowledge the first of its kind to assess the interpretation and evaluation of xAI-generated explanations in the domain of Pathology by the actual target audience. With this, we hope to provide valuable insights for ML researchers, AI developers and HCI designers to build better xAI solutions in pathology, and hope that our research can serve as a template for other efforts in the are of healthcare and beyond. 

<!--Could also be used in the conclusion, what do you think?-->

We find that Pathologists see AI solutions as extension of their regular toolset, with the uppermost goal of saving time for the Pathologist. According to this goal, we find that Pathologists prefer simple and especially visual explanations. Adoption of these solutions is tied both to the expected reduction in workload as well as trust in the AI solution. In the end, Pathologists are responsible for their diagnosis, and if double-checking the AI solutions output is required, this could eat up time savings.
An important aspect of building aspect in AI solutions is to show a decision-making process the Pathologist can relate to, with factors the user can understand. Often, visual explanations facilitate this. When explanations exhibits signs that lead the Pathologist to believe a similar decision making process underlies the algorithm, they tend to increasingly trust in the AI solutions. 
A second factor Pathologists pay attention to is the quality of training data. Detailed information on the annotation expert, the institute and the amount of variance of experts was required. 
However, both the apparent similarity in decision making and quality of training data can be misleading, and the trust is not always justified.
Our proposed new approach for xAI solutions integrates examples from the training data into classifications, leaving the analysis of the complex feature-spaces to the expert.