## potential reviewers

Liron PANTANOWITZ, Prof. Dr. Department of Pathology, University of Pittsburgh, PA, USA
lpantanowitz@gmail.com

Claes LUNDSTROEM, Prof. Dr. Department of Science and Technology, Linköping University, SE
claes.lundstrom@liu.se

Holzinger Notes 30.07.21:
-Figure 1 is good, increase resolution for images, remark on zoomable image
-include more information on how saliency maps was created
-include a few lines on how the AI image was created
-merge figures 2 and 3 with image from 2 shown as example in 3
-figure 4 group cf 1 ax cf 2 ax
-graphical abstract?

Discussion - Central Points:
-treat cautiously when presenting explanations, combination of great possibility
-can build trust in results that deserve that, but can also instill it when undeserved or be misinterpreted
-Trustworthiness, Interpretability
-Trustworthiness and Interpretability of xAI solutions
-how to enable trustworthy AI, trust as main point

%   @article{holzinger_what_2017,
% 	title = {What do we need to build explainable {AI} systems for the medical domain?},
% 	url = {http://arxiv.org/abs/1712.09923},
% 	abstract = {Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.},
% 	urldate = {2020-06-14},
% 	journal = {arXiv:1712.09923 [cs, stat]},
% 	author = {Holzinger, Andreas and Biemann, Chris and Pattichis, Constantinos S. and Kell, Douglas B.},
% 	month = dec,
% 	year = {2017},
% 	note = {arXiv: 1712.09923},
% 	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence},
% 	annote = {Comment: This is a survey article and section 3.1. draws heavily from arXiv:1706.07979},
% 	file = {arXiv Fulltext PDF:/Users/theoevans/Zotero/storage/R9ZYXJ2W/Holzinger et al. - 2017 - What do we need to build explainable {AI} systems fo.pdf:application/pdf;arXiv.org Snapshot:/Users/theoevans/Zotero/storage/H2NZK9AG/1712.html:text/html}
%}

% @InProceedings{soltysinski2007HCI,
% author={Soltysinski, Tomasz},
% editor={Duffy, Vincent G.},
% title={Novel Methods for Human-Computer Interaction in Multimodal and Multidimensional Noninvasive Medical Imaging},
% booktitle={Digital Human Modeling},
% year={2007},
% publisher={Springer Berlin Heidelberg},
% address={Berlin, Heidelberg},
% pages={717--726},
% abstract={The newly developed method for medical noisy data segmentation for the purpose of presentation and supporting the diagnostics is introduced. It also allows for morphometry and visualization of medical multimodal and dynamical data. A general mathematical framework is presented and characterized together with numerous applications. As this tool is designed to support human-computer interaction by means of involving the sense of sight, and suspected to be worthy in the virtual environment sensitive to the sense of touch, the discussion is supported with numerous examples of visualizations and multimodal and multidimensional applications of proposed method.},
% %isbn={978-3-540-73321-8}
% }

% @InProceedings{Golniketal2008micropolar,
% author={Golnik,  Andrzej and Golnik, Natalia and Pałko, Tadeusz and Sołtysiński, Tomasz},
% editor={},
% title={Micro-polarimetry for pre-clinical diagnostics of pathological changes in human tissues},
% booktitle={PROCEEDINGS OF SPIE, 7008},
% year={2008},
% publisher={SPIE},
% pages={70081X}
% }

% @InProceedings{Soltysinski2006EEG,
% author={Sołtysiński, Tomasz and Niedbalski, Paweł},
% editor={},
% title={Towards feature space representation of EEG signals affected by medical treatment},
% booktitle={IFMBE Proceedings, vol. 11, 2534-1-6},
% year={2006},
% publisher={IFMBE},
% pages={70081X}
% }