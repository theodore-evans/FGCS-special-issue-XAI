\section{Discussion}
\label{sec:Discussion}

\subsection{Summary}
This paper set out to answer the following research questions:

\researchquestions

% \begin{enumerate}
%     \item How are examples of prominent xAI approaches interpreted and evaluated by pathologists in the context of a a common AI-assisted diagnostic task.
%     \item What can these interpretations tell us about the strengths, limitations and risks associated with different classes of explanation, and with explanations in general.
% \end{enumerate}    

To address these questions, we invited pathologists from research and clinical practice to take part in an online questionnaire, along with six semi-structured video interviews. These pathologists were presented with examples from five prominent categories of xAI approach, each having the goal of explaining the result of a sample AI solution, and asked to rate and/or discuss their interpretations and the usability of each. 

\subsection{Interpretation of findings}

The study context was chosen to be narrow enough to allow for a meaningful application-grounded study, whilst provide insights as broadly applicable. It is these overarching findings, relating to guiding principles for xAI development, upon which we focus the discussion.

The participants in the study placed a high value on the simplicity of explanation methods. While this could be understood in terms of a general user preferences, it is important to view this in the context of the pathologists' fast-paced routine workflow. In this setting, saving time was considered to be the most valuable benefit of AI assistance. Explanations involving complex interface elements, and therefore requiring additional scrutiny, may serve to undermine this purpose. The preference for visual explanations can also be seen in this light, with the introduction of different modalities into the primarily visual workflow imposing unwanted additional cognitive load.

% >> implications: in conjunction with other points > conflict

With respect to the usability of explanation methods, a broad range of modalities were identified as useful \textit{sanity checks} -- i.e. those flagging instances of the AI doing something `clearly wrong'. Many situations were identified where it would be straightforward for an explanation to discredit the AI system, for example: 

\begin{itemize}
    \item A saliency map highlighting an irrelevant part of the image
    \item Concept attribution omitting critical diagnostic features
    \item Prototypes significantly deviating from ``textbook examples''
\end{itemize}

% >> integration: exploration of decision space, must be something there. target group of explanations, arrieta has some work here, many more xAI approaches applicable to developers vs users zB
% >> implications: research landscape already caters strongly for developers, to develop xAI for users is far more nuanced, challenging and cross-disciplinary

In comparison, explanations providing users with a more nuanced understanding of the decision-making factors of the AI system occupied a smaller subset of potential implementations. Above all else, the acceptability and perceived value of such explanations was determined by the degree to which they allowed the user to infer causality in the AI system's reasoning. In particular, there was a greater readiness to place trust in AI systems whose explanations seemed to imply causal decision-making processes that were similar to the users' own.

% >> integration: all of the research on explanations displaying causality, the HCI papers
% >> implications: valuable types of explanation approaches implied/proposed by participants based upon the examples presented, including: Counterfactual explanations showing model classification changing as a function of identifiable, diagnostically meaningful visual features, prototypes representing decision space, prototypes representing finely split clusters (therefore also close to decision boundary, BUT also conflict with simplicity, etc.

Moreover, participants showed a clear propensity to ascribe relatable (both human- and pathologist-like) causal reasoning to the AI system in order to try and understand the explanations presented. This was particularly noteworthy in examples that were not intended to convey such information. For example, the inference of causal factors determining high and low trust scores was readily used as a basis for perceived trustworthiness of results -- in the vein of ``the AI seemed to have the same difficulties that I would''.

This tendency was recognised as introducing significant risk of bias, particularly as a function of the omission or ambiguity of information presented by explanations. %This effect was emphasised in cases where participants made questionable inferences about the reasoning capabilities of the AI system based on the ambiguous explanation presented. Only during discussion of these results (and in some cases, not at all) were the errors in reasoning identified -- errors that that may have passed unnoticed in a real-world situation. 

For example

\begin{itemize}
    \item Saliency maps that highlight diagnostically relevant regions, whilst leaving unclear which features caused those regions to be relevant
    \item Counterfactuals in which multiple features simultaneously change, leaving it ambiguous as to which were truly relevant
\end{itemize}

The tendency for positive confirmation bias was particularly notable when explanations presented information that closely matched pathologist expectations. An example of such thinking: that a prototype representing features correctly associated with a Ki-67 positive nucleus implied that the AI system was using the correct features to make this class assessment. In reality, the prototypical case gives no information about how varied or inclusive this class may be within the decision space of the AI system, or where and based on what features the AI system draws this class boundary.

% >> integration: something from the HCI/social/cognitive research on xAI
% >> implications: conflict, risk postive confirmation bias

Another trust-building element in interaction between pathologist and xAI system was the ability to `get to know' an AI system: To explore its limitations and quirks, with explanation=generating mechanisms providing an interface surface for this interaction. This \textit{trial period} was identified as an essential component in the building of pathologists' trust in AI systems. 

The flavour of this interaction varied with attitude toward the role of AI in diagnostic decision-making. Pathologists who were open to the potential of AI systems to play a collaborative role with their users expected explanations to enable them to probe their capabilities like they would a trainee or colleague. On the other hand, those considering AI assistance more as in the way of a `dumb machine' tended to treat explainability elements as an interface for manual adjustment of AI results.

% >> integration: ? attitudes to AI
% >> implications: interactivity

This split also reflected attitudes toward human versus machine fallibility. The AI-skeptical considered deviations between human and AI assessments to be cause for rejecting the system. On the other side, conflicting AI results, even one turning out to be incorrect, were considered by some to be valuable for keeping the pathologist `on their toes'. In these cases, the AI's indifference toward experience or seniority was deemed an attractive attribute, one often missing from second opinions provided by human colleagues.

% >> integration: Jussupow paper
% >>implications, very different needs depending on attitudes toward AI, experience of AI effectiveness, personality. and attitudes toward AI very diverse due to patchy, limited exposure, often to flawed research implementations. likely to remain so until it reaches a certain level of standardisation and adoption (which in turn limited by digital adoption. Difficult problem for building xAI systems.

Alongside explainability, the traceability and credibility of data and quality assurance methods used for training and validation was identified as a critical factor for building pathologist trust in AI systems.

\subsection{Implications for xAI research}

% The difficulty of conducting this type of grounded study on xAI systems themselves, as we have done, are the multiple layers of novelty involved. As well as being exposed for the first time to many of the explanation generation techniques presented, participants experience with AI systems in general was also limited. >> limitations

% >> integration: exploration of decision space, must be something there. target group of explanations, Arrieta has some work here, many more xAI approaches applicable to developers vs users zB

Our findings strongly support observations that social and cognitive biases strongly affect human interactions with xAI systems~\cite{miller2019explanation, jussupow2021augmenting}, and that an important aspect of this is the tendency to ascribe human-like traits to explainable AI systems~\citet{de2017people, miller2019explanation}. This emphasises the importance of designing xAI systems that identify, mitigate, and even take advantage of, such biases and predispositions.

The sources and effects of bias evident in our study support many of the observations and strategies for user-centric xAI systems proposed by \citet{wang_designing_2019}. It should be noted that, while these guidelines are theory-driven, we have reached many of the same conclusions based on purely empirical findings, lending credibility to this prior work.

% These include
% - static explanations beg their own questions of transparency
% - interactivity both gives the user the feeling of being in control
% - allows them to dig as deep as they need to for an explanation | support HD reasoning | correct for the context and audience sensitive nature of explanations
% - contrastive explanations show what might have been, visual counterfactuals, semantic concepts, and in particular, combinations of the two
% - prototypes that sample over the configuration of a class, indicating common features uniting a cluster
% - prototypes that show centroids of nearby clusters (and therefore lay close to the decision boundary between the two)

In addition to the role of explainability in building pathologist trust in AI systems, the need to complement such implementations with model and data transparency was reinforced. The implementation of this suggested by users is reminiscent of the ``model facts" label proposed by\cite{sendak2020presenting}, containing: A summary of the model, training and inference methods, validation and performance metrics, as well as uses and warnings, all presented in standardised and user-accessible language. Beyond these desiderata, pathologists in our study emphasised the importance of understanding sources of model training data: in particular with regards to the credibility and variety of expert annotators.

Our study was motivated in large part by the predominance of algorithmic (rather than user-centric) approaches to xAI in the state of the art~\cite{tjoa_survey_2020, poceviciute_survey_2020, antoniadi2021current}. We agree with the observation of \citet{miller2019explanation} that most of this work ``uses only the researchers' intuition of what constitutes a ‘good’ explanation''. While studies to identify user requirements for explainability represents a marked improvement on this, our findings underline the importance of the extra step we have taken: evaluating user interactions with potential xAI approaches directly.

Specifically, our study implies some difficult-to-avoid conflicts that may arise between explainability requirements perceived by user and the potential biases that these may be liable to introduce.

\subsubsection{Cognitive load vs. Positive confirmation bias}

The time pressure on pathologists during routine examination places a high burden on AI systems and their explainable counterparts to provide UI elements that introduce minimal cognitive load. Preferences expressed by users in this study were for simple, visual elements, with the simplest explanation candidates (prototypes) being considered the most intuitively understandable, and in many cases, the most readily accepted.

% can't get wording for start of paragraph:
However, it was evident in this study that every detail omitted from an explanation, whether for the sake of simplicity or due to inherent limitation, is an opportunity for speculation. That is to say, that any gaps or ambiguities in the information presented to the user are liable to be ``filled in'' according to their own expectations and understanding. This introduces a strong risk of confirmation bias, whereby a user is lulled into a false sense of security by explanations that underdetermine the factors important for a model output. 

% One of these, An inherent conflict between reducing cognitive load and introducing misleading bias
% SUCH THAT
% making explanations simple means that every detail left out of an explanation is subject to speculation | a gap to liable to be 'filled in' by the user, inevitably according to their own expectations and understanding
% - and this is even observable with explanations not designed to reveal causality (e.g. TS)

\subsubsection{Relatability vs. Anthropomorphism}

The expectation of relatable explanations, or rather, of explanations that represent relatable AI reasoning, presents a similar challenge. 

making explanations relatable -- i.e. "speak the language of the user" -- risks implying human-like reasoning of AI systems that work very differently AND make explanations too reliant on user intuition, therefore easy to digest but with implicit, potentially ambiguous / misleading informational content. 

The most honest explanations of AI outputs are abstract, reflecting the abstraction from human-like causal reasoning that the current generation of machine learning techniques employ.

Address both from xAI design and from a free exchange of domain knowledge between machine learning experts and the potential users of AI systems. 

\subsubsection{Human-computer interaction as first-class citizen}

there may be many more algorithmic approaches for xAI. As demonstrated in our findings, a broad range of these may all be useful for raising red flags. Indeed, this rationale for xAI solutions already serves to much of the research landscape (cite Clever Hans problem). Given the mounting pressure, in the domains of medical decision support and more generally, to `add explainability' to AI systems, there is the risk that such methods are integrated without taking into consideration unforeseen second-order effects

but in light of the potential for unintended consequences, it is important to weigh the inclusion of explainability elements into AI systems carefully, and to work in a close loop with users, whose feedback should be moderated by cognitive and social perspectives. 

With regards to the development and integration of explainability into AI systems, we urge developers to consider the cautionary words of Dr. Ian Malcolm: to not be so preoccupied with whether or not you can, that you do not stop to think whether you should~\cite{crichton1991jurassic}.

and to continue looking to the users, as well as cross-disciplinary motivations for new approaches to explanation-generation.
  
\subsection{Limitations}

% Due to the limited availability of qualified pathologists to take part in research, particularly those involved in routine diagnostic work, only a relatively small sample size of 25 questionnaire respondents and six interview participants was reached. While this sample size does not enable strong statistical conclusions to be drawn, we believe that it allows a representative overview of the target audience, particularly given the qualitative nature of the findings presented. >> due to the small sample size, some aspects may have been overrepresented, while others overlooked

% Given the limited availability of pathologists, the research apparatus was designed with brevity in mind. The resulting xAI examples were therefore simplified and mostly static. This deviates significantly from the expected user experience of a pathologist in a diagnostic workflow. A similar study framed within a more representative and immersive user experience would provide valuable insights, whilst significantly increasing the technical cost of study implementation.

% % Specific points included the inability to view the non-annotated IHC image, adjacent H\&E layers, and generally pan and zoom through the slide data. 

% % The lack of clarity on the output of the AI solution (insufficient context, unclear interpretation for unclassified nuclei) was a potentially confounding factor in the pathologists' interpretation of these results. While these results in and of themselves were not the subject of this research, it cannot be ruled out that this had a indirect effect of the interpretation of the explanations of which they were the subject. Given that this limitation was consistent across all explanations examples presented, we suggest that this limitation does not strongly impact the validity of comparative statements between different classes of explanation. It is also suggested that this shortfall is somewhat representative of issues with real AI solutions and provides a valuable discussion point regarding the value and usability of explanations. 

% Potentially, a more significant limitation of this study lies with the choice, number and design of explanations examples. To compare these examples within a common context, it was necessary to create most by hand, with only the saliency map implementations created with a true algorithmic approach. This was necessitated by the fact that many of these explanations were based on speculative techniques, either currently unsupported by any technical implementation or with implementations that are not feasible for application to the diagnostic context without incurring prohibitive technical overhead. Although the creation of these examples was supported with the help of the expertise of a broad group of stakeholders, both from domains of ML and pathology, there is the chance that some aspects may not be representative of realistic explanations. Most importantly, the saliency map examples are subject to the limitations of a real xAI implementation, which may put them at an inherent disadvantage  compared with the other, idealised examples. 
% >> i.e. the explanations were created as a well-informed guess. True implementations of xAI may have had different results

% the risk of positive confirmation bias may be over-represented due to the setup of the study. The use case of Ki-67 quantification has relatively few diagnostic factors that play an important role in nuclear classification, in particular, if the identification of the tumor region has already taken place in an earlier part of the examination or AI pipeline. Therefore, by asking participants to talk about the factors they can infer from the explanations given, we may have introduced an incentive to read too much into the elements displayed. 

% FW>> We suggest that more, xAI elements applied to a more complex task, where xAI elements are more appropriate for weighing up between the many diagnostic factors that may play a role

% Another approach that was not tested is potential over-complexity and distraction capacity of the tested xAI approaches. It is clear that unhelpful explanations can produce more harm than good by distracting or overwhelming the user with unnecessary information. We therefore recommend further research which should explicitly measure the impact of explanations and compare them with a baseline to ensure that actual explanatory value is derived for the end user while keeping the required mental load to a minimum and thereby reducing distractions. >> explicitly addressed in discussion



\section{Conclusion}
% \label{sec:Conclusion}

THEREFORE

by making explanations simple and/or intuitively relatable, we leave degrees of freedom open to interpretation, we risk reinforcing the idea that the user's understanding can be projected onto the AI system and/or introducing positive confirmation bias 
BUT
if we make explanations less simple and more expressive, we reduce their usefulness to and acceptance by pathologists

WITH THE PROVISO: that the lack of of 'sweet spot' in this balance could be a function of the use case chosen, demonstrating that explanations really are redundant for this task (interesting as a result, and as a methodology?)

THEREFORE
it is better to be explicit than implicit
just because you can, doesn't mean you should!
content and modality of information communicated to the user should be a first class citizen in AI system design

% If, through this process, no balance can be found between the usability of xAI solutions and the potential sources of bias they may incur, it may be a sign that either explainability is inappropriate for this case, or that a more fundamental redesign of the system is in order. The takeaway message here being, that it is not possible to `add explainability' in as an afterthought to AI development (as is, in the experience of the authors, often the case). Rather, that the informational content and communication modality required for the user of an AI system, in order that they are able to take the best decision possible, are considerations that must be built into the development from step one. >> conclusion

xAI to be designed with a strong emphasis on the social and cognitive aspects of user interaction, with user preferences and feedback also viewed through this lens

% We believe that the preference demonstrated by pathologists in our findings for some explanation classes over others can, to a certain extent, be taken at face value. However, our findings also imply various second-order and unintended effects associated with particular examples that should be considered when interpreting their order of preference. 


\section{Future work}
\label{sec:FutureWork}

