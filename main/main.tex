% Template as of 22.04.2021 error free
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper,5p,review]{elsarticle}
%\documentclass[final,5p,times,twocolumn]{elsarticle}

\usepackage{lineno}
\usepackage{easylist}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage[breaklinks]{hyperref}
\usepackage{url}
\usepackage{textcomp}
\usepackage{verbatim}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{ulem}
%\usepackage[ampersand]{easylist}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{listings} 
\pgfplotsset{compat=1.14}
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}
\usepackage{tikzscale}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usepackage{tikz-qtree,tikz-qtree-compat}
\usetikzlibrary{calc}
\modulolinenumbers[5]

\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

\pdfstringdefDisableCommands{%
  \def\corref#1{}%
}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}

\title{Crisp, Short, Clear, Concise Title of the Paper}

%% or include affiliations in footnotes:
\author[TUB]{Theodore Evans\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{theodor.evans@dai-labor.de}
\author[TUB]{Christian Geissler}
\author[TUB]{Carl Ogre Retzlaff}
\author[CAR]{Norman Zerbe}
\author[xxx]{Erika Musterfrau}
\author[xxx]{Max Mustermann}
\author[MUG]{Markus Plass}
\author[MUG]{Heimo Mueller}
\author[MUG,amii]{Andreas Holzinger }


\address[TUB]{DAI, Technical University Berlin, Germany}
\address[MUG]{Medical University Graz, Austria}
\address[amii]{Alberta Machine Intelligence Institute, Canada}
\address[xxx]{Lab Name, University Name, Address}

\begin{abstract} 
What was already known on the topic to the international research community ?
%This must be shortened still !! 
% NOTE- PLACEHOLDER TEXT ONLY. This text was pulled directly from the call for papers on Elsevier website. Should be completely rewritten, not just adapted
The spread of the use of artificial intelligence techniques is now ubiquitous and unstoppable. It brings many opportunities and helps solve old problems. However, by its very nature, the use of AI also brings many risks and new problems that must be addressed to avoid jeopardizing effective evolution. The emerging field of eXplainable AI (XAI) is helping to find answers to these problems and put people more at the center.
While from a research perspective, discussions of XAI date back several decades and were reinvigorated by the DARPA initiative, the concept emerged with renewed vigor in late 2019 when Google announced a new XAI toolset for developers after announcing its "AI-first" strategy in 2017. This is because many of today's machine and deep learning applications do not allow us to fully understand how they work or the logic behind them, which is referred to as a "black box." The high complexity makes many successful machine learning models difficult or impossible to understand. This characteristic is considered one of the biggest problems in the application of AI techniques; it makes machine decisions non-transparent and often incomprehensible even to experts or developers. Explainable AI systems can explain the logic of decisions, characterize the strengths and weaknesses of decision making, and provide insights into their future behavior.
What this paper contributes to the international research community ?

This paper describes, analyzes  ...

The novelty is, the results show, the paper demonstrates ...

The benefit is, it indicates that ...



%The spread of the use of artificial intelligence techniques is now pervasive and unstoppable. However, it brings with its opportunities but also risks and problems that must be addressed in order not to compromise an effective evolution. The eXplainable AI (XAI) is one of the answers to these problems to bring humans closer to machines. While from a research perspective the discussions on XAI date back a few decades, the concept emerged with renewed vigour at the end of 2019 when Google, after announcing its "AI-first" strategy in 2017, recently announced a new XAI toolset for developers. Nowadays many of the machine and deep learning applications do not allow you to understand how they work entirely or the logic behind them for effect called "BlackBox", according to which machine learning models are mostly black boxes. This feature is considered one of the biggest problems in the application of AI techniques; it makes machine decisions not transparent and often incomprehensible even to the eyes of experts or developers themselves. Explainable AI systems can explain the logic of decisions, characterize the strengths and weaknesses of decision making, and provide insights into their future behaviour.
\end{abstract}

\begin{keyword}
Explainable AI, interpretable Machine Learning, interactive Machine Learning, aggregation functions, ordinal sums, Glass--box approach, transparency 
\end{keyword}

\end{frontmatter}
\linenumbers

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OVERVIEW

% Motivation
% - Explainability as cross-disciplinary, embedded concept
% - Limitations of focusing on either algorithmic or UX aspects alone
% - Need for deeper, cross-disciplinary study of explainability requirements in a specific context

% Outcome
% - Exploratory case study 
% - Investigating the understandability, informativeness and value to user for
% - n 'explanation classes', with presentation modalities chosen to be appropriate
% - For a representative Ki-67 AI solution, with model outputs of generated annotations and overall nuclear positivity
% - analysed with respect to users' usage / familiarity with AI solutions/ ML in general

% Future work
% - Technical implementations for classes of explanation that are understandable, provide insight and value, but are currently not represented in the SOA

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{\textbf{Highlights}}

\begin{itemize}
    \item This paper describes, analyzes, 
    \item The novelty is, the results show, the paper demonstrates
    \item The benefit is, it indicates that, 
\end{itemize}
%First paper that evaluates XAI-Approaches in the Pathology domain!!!

\include{main/1Introduction}
%Norman, Tomasz

%Ki-67 Use case description
% Rasmus & Norman

\include{main/2StateOfTheArt}
% Tomasz, Andreas, Norman, Christian, Theodore, Carl

%User Interface SotA for medical applications

\include{main/3CaseStudyDesign}
% Theodore, Christian

\include{main/4ResultsAndAnalysis}
% Carl

\include{main/5ConclusionFutureWork}

\bibliography{references}

% \section*{About the Authors}

% \parpic{\includegraphics[width=1in,clip,keepaspectratio]{bio-images/dummy.jpg}}
%\noindent {\bf Author Name} is the most famous author in her field. 
%She is particularly interested in X and Y, and also dabbles in Z.

% \bio{bio-images/evans.png}
% Theodore Evans received his M.Phys. degree in physics from the University of Manchester. He is a Ph.D. Candidate at the Distributed Artificial Intelligence Laboratory (DAI-Labor) of the Technisches Universit√§t Berlin (TU-Berlin). He is currently supervised by Prof. Dr. Dr. hc Sahin Albayrak. His research interests lie in representation learning and cross-disciplinary approaches to explainable AI-assistance for digital pathology.
% \endbio

%\bio{bio-images/dummy.jpg}
%Max Mustermann is senior researcher at the awesome explainability Lab at the wonderful university of dreamland, and he is visiting researcher at the institute paradise in fantasy land. He received his Masters in computer science and her PhD in Computer Science from top university x. Erika is a member of the prestigious club wonder. 
%\endbio

% \bio{bio-images/holzinger.jpg}
% Andreas Holzinger is Visiting Professor for explainable AI at the University of Alberta, Canada since 2019 and head of the Human-Centered AI Lab at the Medical University Graz, Austria. He received his PhD in cognitive science from Graz University and his second PhD in computer science from Graz University of Technology. Andreas is ordinary member in the Academia Europaea, the European Academy of Sciences in the section Informatics, and full member of the European Lab for Learning and Intelligent Systems.
% \endbio

\end{document}


