% references sorted in alphabetical order
% exception: 
%references related to the regulatory landscape are appended at the end 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAA

@article{acs2020artificial,
  title={Artificial intelligence as the next step towards precision pathology},
  author={Acs, Bal{\'a}zs and Rantalainen, Mattias and Hartman, Johan},
  journal={Journal of internal medicine},
  volume={288},
  number={1},
  pages={62--81},
  year={2020},
  publisher={Wiley Online Library}
}

@article{antoniadi2021current,
  title={Current challenges and future opportunities for {XAI} in machine learning-based clinical decision support systems: a systematic review},
  author={Antoniadi, Anna Markella and Du, Yuhan and Guendouz, Yasmine and Wei, Lan and Mazo, Claudia and Becker, Brett A and Mooney, Catherine},
  journal={Applied Sciences},
  volume={11},
  number={11},
  pages={5088},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{arrieta2020explainable,
  title={Explainable Artificial Intelligence ({XAI}): Concepts, taxonomies, opportunities and challenges toward responsible {AI}},
  author={Arrieta, Alejandro Barredo and D{\'\i}az-Rodr{\'\i}guez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc{\'\i}a, Salvador and Gil-L{\'o}pez, Sergio and Molina, Daniel and Benjamins, Richard and others},
  journal={Information Fusion},
  volume={58},
  pages={82--115},
  year={2020},
  publisher={Elsevier}
}

@article{asan:trustmedicalai:2020,
author="Asan, Onur
and Bayrak, Alparslan Emrah
and Choudhury, Avishek",
title="Artificial Intelligence and Human Trust in Healthcare: Focus on Clinicians",
journal="J Med Internet Res",
year="2020",
month="Jun",
day="19",
volume="22",
number="6",
pages="e15154",
keywords="human-AI collaboration; trust; technology adoption; FDA policy; bias; health care",
issn="1438-8871",
doi="10.2196/15154",
abstract={Artificial intelligence (AI) can transform health care practices with its increasing ability to translate the uncertainty and complexity in data into actionable—though imperfect—clinical decisions or suggestions. In the evolving relationship between humans and AI, trust is the one mechanism that shapes clinicians’ use and adoption of AI. Trust is a psychological mechanism to deal with the uncertainty between what is known and unknown. Several research studies have highlighted the need for improving AI-based systems and enhancing their capabilities to help clinicians. However, assessing the magnitude and impact of human trust on {AI} technology demands substantial attention. Will a clinician trust an AI-based system? What are the factors that influence human trust in AI? Can trust in {AI} be optimized to improve decision-making processes? In this paper, we focus on clinicians as the primary users of {AI} systems in health care and present factors shaping trust between clinicians and AI. We highlight critical challenges related to trust that should be considered during the development of any {AI} system for clinical use.}
}

%%% BBB

@inproceedings{kim2018interpretability,
  title={Interpretability beyond feature attribution: Quantitative testing with concept activation vectors ({TCAV})},
  author={Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and others},
  booktitle={International conference on machine learning},
  pages={2668--2677},
  year={2018},
  organization={PMLR}
}


@article{BodenLundstrom:2021:HumanPatho,
   year = {2021},
   author = {Bodén, Anna C.S. and Molin, Jesper and Garvin, Stina and West, Rebecca A. and Lundström, Claes and Treanor, Darren},
   title = {The human‐in‐the‐loop: an evaluation of pathologists’ interaction with artificial intelligence in clinical practice},
   journal = {Histopathology},
   volume = {79},
   number = {2},
   pages = {210--218},
   abstract = {One of the major drivers of the adoption of digital pathology in clinical practice is the possibility of introducing digital image analysis (DIA) to assist with diagnostic tasks. This offers potential increases in accuracy, reproducibility, and efficiency. Whereas stand-alone DIA has great potential benefit for research, little is known about the effect of DIA assistance in clinical use. The aim of this study was to investigate the clinical use characteristics of a DIA application for Ki67 proliferation assessment. Specifically, the human-in-the-loop interplay between DIA and pathologists was studied. Methods and results We retrospectively investigated breast cancer Ki67 areas assessed with human-in-the-loop DIA and compared them with visual and automatic approaches. The results, expressed as standard deviation of the error in the Ki67 index, showed that visual estimation (‘eyeballing’) (14.9 percentage points) performed significantly worse (P < 0.05) than DIA alone (7.2 percentage points) and DIA with human-in-the-loop corrections (6.9 percentage points). At the overall level, no improvement resulting from the addition of human-in-the-loop corrections to the automatic DIA results could be seen. For individual cases, however, human-in-the-loop corrections could address major DIA errors in terms of poor thresholding of faint staining and incorrect tumour–stroma separation. Conclusion
The findings indicate that the primary value of human-in-the-loop corrections is to address major weaknesses of a DIA application, rather than fine-tuning the DIA quantifications.},
   doi = {10.1111/his.14356}
}



@article{bodria_benchmarking_2021,
	title = {Benchmarking and {Survey} of {Explanation} {Methods} for {Black} {Box} {Models}},
	volume = {2102},
	url = {http://adsabs.harvard.edu/abs/2021arXiv210213076B},
	abstract = {The widespread adoption of black-box models in Artificial Intelligence has enhanced the need for explanation methods to reveal how these obscure models reach specific decisions. Retrieving explanations is fundamental to unveil possible biases and to resolve practical or ethical issues. Nowadays, the literature is full of methods with different explanations. We provide a categorization of explanation methods based on the type of explanation returned. We present the most recent and widely used explainers, and we show a visual comparison among explanations and a quantitative benchmarking.},
	urldate = {2021-05-20},
	journal = {arXiv e-prints},
	author = {Bodria, Francesco and Giannotti, Fosca and Guidotti, Riccardo and Naretto, Francesca and Pedreschi, Dino and Rinzivillo, Salvatore},
	month = feb,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	pages = {arXiv:2102.13076},
	annote = {Submitted to CORR, currently under review},
	file = {Full Text PDF:files/848/Bodria et al. - 2021 - Benchmarking and Survey of Explanation Methods for.pdf:application/pdf}
}

@article{brooke1996sus,
  title={{SUS}-A quick and dirty usability scale},
  author={Brooke, John and others},
  journal={Usability evaluation in industry},
  volume={189},
  number={194},
  pages={4--7},
  year={1996},
  publisher={London--}
}

%%% CCC

@article{cai2019hello,
author = {Cai, Carrie J. and Winter, Samantha and Steiner, David and Wilcox, Lauren and Terry, Michael},
title = {"{Hello AI}": Uncovering the Onboarding Needs of Medical Practitioners for {Human-AI} Collaborative Decision-Making},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359206},
doi = {10.1145/3359206},
abstract = {Although rapid advances in machine learning have made it increasingly applicable to
expert decision-making, the delivery of accurate algorithmic predictions alone is
insufficient for effective human-AI collaboration. In this work, we investigate the
key types of information medical experts desire when they are first introduced to
a diagnostic {AI} assistant. In a qualitative lab study, we interviewed 21 pathologists
before, during, and after being presented deep neural network (DNN) predictions for
prostate cancer diagnosis, to learn the types of information that they desired about
the {AI} assistant. Our findings reveal that, far beyond understanding the local, case-specific
reasoning behind any model decision, clinicians desired upfront information about
basic, global properties of the model, such as its known strengths and limitations,
its subjective point-of-view, and its overall design objective--what it's designed
to be optimized for. Participants compared these information needs to the collaborative
mental models they develop of their medical colleagues when seeking a second opinion:
the medical perspectives and standards that those colleagues embody, and the compatibility
of those perspectives with their own diagnostic patterns. These findings broaden and
enrich discussions surrounding {AI} transparency for collaborative decision-making,
providing a richer understanding of what experts find important in their introduction
to {AI} assistants before integrating them into routine practice.},
journal = {ACM Transactions on Computer-Human Interaction.},
month = nov,
articleno = {104},
numpages = {24},
keywords = {human-ai interaction, clinical health, machine learning}
}

@inproceedings{cao2009modality,
  title={Modality effects on cognitive load and performance in high-load information presentation},
  author={Cao, Yujia and Theune, Mari{\"e}t and Nijholt, Anton},
  booktitle={Proceedings of the 14th international conference on Intelligent user interfaces},
  pages={335--344},
  year={2009}
}

@article{Carrington:2021:DeepROC,
   year = {2021},
   author = {Carrington, Andre M. and Manuel, Douglas G. and Fieguth, Paul W. and Ramsay, Tim and Osmani, Venet and Wernly, Bernhard and Benett, Carol and Hawken, Steven and McInnes, Matthew and Magwood, Olivia and Sheikh, Yusuf and Holzinger, Andreas},
   title = {Deep ROC Analysis and AUC as Balanced Average Accuracy to Improve Model Selection, Understanding and Interpretation},
   journal = {arXiv preprint arXiv:2103.11357}
}

@article{Castelvecchi:2016:OpenBlack,
   year = {2016},
   author = {Castelvecchi, Davide},
   title = {Can we open the black box of {AI}?},
   journal = {Nature News},
   volume = {538},
   number = {7623},
   pages = {20-23},
   doi = {10.1038/538020a}
}

@inproceedings{chakraborti_emerging_2020,
	title = {The {Emerging} {Landscape} of {Explainable} {Automated} {Planning} \&amp; {Decision} {Making}},
	volume = {5},
	url = {https://www.ijcai.org/proceedings/2020/669},
	doi = {10.24963/ijcai.2020/669},
	abstract = {Electronic proceedings of IJCAI 2020},
	language = {en},
	urldate = {2021-04-27},
	author = {Chakraborti, Tathagata and Sreedharan, Sarath and Kambhampati, Subbarao},
	month = jul,
	year = {2020},
	note = {issn: 1045-0823},
	pages = {4803--4811},
	booktitle={IJCAI},
	file = {Full Text PDF:/Users/theoevans/Zotero/storage/35CU5I2I/Chakraborti et al. - 2020 - The Emerging Landscape of Explainable Automated Pl.pdf:application/pdf}
}

@article{Corazzini:1977:trust,
   year = {1977},
   author = {Corazzini, John G.},
   title = {Trust as a complex multi-dimensional construct},
   journal = {Psychological Reports},
   volume = {40},
   number = {1},
   pages = {75-80},
   abstract = {While many uses of the construct trust assume unidimensionality, there is a lack of agreement as to meaning and measurement. Some have suggested trust to be a multi-dimensional construct. This study explored further the notion that trust is multi-dimensional. A factor analysis of three paper-and-pencil measures of trust yielded four underlying factors: suspicion-trust, risk-taking, gambling, and cynicism. The implications of this finding are discussed. Recommendations for future conceptualization and study are included.},
   doi = {10.2466/pr0.1977.40.1.75}
}



@article{Chu2020visualexplanations,
  title={Are Visual Explanations Useful? A Case Study in Model-in-the-Loop Prediction},
  author={Eric Chu and D. Roy and Jacob Andreas},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.12248}
}

@article{cui2021artificial,
  title={Artificial intelligence and computational pathology},
  author={Cui, Miao and Zhang, David Y},
  journal={Laboratory Investigation},
  volume={101},
  number={4},
  pages={412--422},
  year={2021},
  publisher={Nature Publishing Group}
}


%%% DDD

@article{das2020computer,
  title={Computer-aided histopathological image analysis techniques for automated nuclear atypia scoring of breast cancer: a review},
  author={Das, Asha and Nair, Madhu S and Peter, S David},
  journal={Journal of digital imaging},
  volume={33},
  number={5},
  pages={1091--1121},
  year={2020},
  publisher={Springer}
}

@article{dowsett2011assessment,
  title={Assessment of Ki67 in breast cancer: recommendations from the International Ki67 in Breast Cancer working group},
  author={Dowsett, Mitch and Nielsen, Torsten O and A’Hern, Roger and Bartlett, John and Coombes, R Charles and Cuzick, Jack and Ellis, Matthew and Henry, N Lynn and Hugh, Judith C and Lively, Tracy and others},
  journal={Journal of the National cancer Institute},
  volume={103},
  number={22},
  pages={1656--1664},
  year={2011},
  publisher={Oxford University Press},
  doi={10.1093/jnci/djr393}
}

%%% FFF

@article{Feldman:2000:Nature,
   year = {2000},
   author = {Feldman, J.},
   title = {Minimization of Boolean complexity in human concept learning},
   journal = {Nature},
   volume = {407},
   number = {6804},
   pages = {630-633},
   abstract = {One of the unsolved problems in the field of human concept learning concerns the factors that determine the subjective difficulty of concepts: why are some concepts psychologically simple and easy to learn, while others seem difficult, complex or incoherent? This question was much studied in the 1960s(1) but was never answered, and more recent characterizations of concepts as prototypes rather than logical rules(2,3) leave it unsolved(4-6). Here I investigate this question in the domain of Boolean concepts (categories defined by logical rules). A series of experiments measured the subjective difficulty of a wide range of logical varieties of concepts (41 mathematically distinct types in six families-a for wider range than has been tested previously). The data reveal a surprisingly simple empirical 'law': the subjective difficulty of a concept is directly proportional to its Boolean complexity (the length of the shortest logically equivalent propositional formula)-that is, to its logical incompressibility.},
   doi = {10.1038/35036586}
}

@article {Ferrario:trustmedicalai,
	author = {Ferrario, Andrea and Loi, Michele and Vigan{\`o}, Eleonora},
	title = {Trust does not need to be human: it is possible to trust medical {AI}},
	volume = {47},
	number = {6},
	pages = {437--438},
	year = {2021},
	doi = {10.1136/medethics-2020-106922},
	publisher = {Institute of Medical Ethics},
	abstract = {In his recent article {\textquoteleft}Limits of trust in medical AI,{\textquoteright} Hatherley argues that, if we believe that the motivations that are usually recognised as relevant for interpersonal trust have to be applied to interactions between humans and medical artificial intelligence, then these systems do not appear to be the appropriate objects of trust. In this response, we argue that it is possible to discuss trust in medical artificial intelligence (AI), if one refrains from simply assuming that trust describes human{\textendash}human interactions. To do so, we consider an account of trust that distinguishes trust from reliance in a way that is compatible with trusting non-human agents. In this account, to trust a medical {AI} is to rely on it with little monitoring and control of the elements that make it trustworthy. This attitude does not imply specific properties in the {AI} system that in fact only humans can have. This account of trust is applicable, in particular, to all cases where a physician relies on the medical {AI} predictions to support his or her decision making.},
	issn = {0306-6800},
	URL = {https://jme.bmj.com/content/47/6/437},
	eprint = {https://jme.bmj.com/content/47/6/437.full.pdf},
	journal = {Journal of Medical Ethics}
}


%%% GGG

@article{garcia2019new,
  title={New {European Union} regulations related to whole slide image scanners and image analysis software},
  author={Garcia-Rojo, Marcial and De Mena, David and Muriel-Cueto, Pedro and Atienza-Cuevas, Lidia and Dominguez-Gomez, Manuel and Bueno, Gloria},
  journal={Journal of pathology informatics},
  volume={10},
  year={2019},
  publisher={Wolters Kluwer--Medknow Publications}
}

@article{gaube:trustmedicalai:2021,
  title={Do as {AI} say: susceptibility in deployment of clinical decision-aids},
  author={Gaube, Susanne and Suresh, Harini and Raue, Martina and Merritt, Alexander and Berkowitz, Seth J. and Lermer, Eva and Coughlin, Joseph F. and  Guttag, John V. and Colak, Errol and Ghassemi, Marzyeh},
  journal={npj Digital Medicine},
  volume={4},
  number={31},
  year={2021},
  doi={10.1038/s41746-021-00385-9},
  abstract = {Artificial intelligence (AI) models for decision support have been developed for clinical settings such as radiology, but little work evaluates the potential impact of such systems. In this study, physicians received chest X-rays and diagnostic advice, some of which was inaccurate, and were asked to evaluate advice quality and make diagnoses. All advice was generated by human experts, but some was labeled as coming from an {AI} system. As a group, radiologists rated advice as lower quality when it appeared to come from an {AI} system; physicians with less task-expertise did not. Diagnostic accuracy was significantly worse when participants received inaccurate advice, regardless of the purported source. This work raises important considerations for how advice, {AI} and non-AI, should be deployed in clinical environments.}
}

@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group},
  doi={10.1038/s42256-020-00257-z}
}

@article{geread2021pinet,
  title={{piNET}--An Automated Proliferation Index Calculator Framework for Ki67 Breast Cancer Images},
  author={Geread, Rokshana Stephny and Sivanandarajah, Abishika and Brouwer, Emily Rita and Wood, Geoffrey A and Androutsos, Dimitrios and Faragalla, Hala and Khademi, April},
  journal={Cancers},
  volume={13},
  number={1},
  pages={11},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute},
  doi={10.3390/cancers13010011}
}

@article{govind2020improving,
  title={Improving the accuracy of gastrointestinal neuroendocrine tumor grading with deep learning},
  author={Govind, Darshana and Jen, Kuang-Yu and Matsukuma, Karen and Gao, Guofeng and Olson, Kristin A and Gui, Dorina and Wilding, Gregory E and Border, Samuel P and Sarder, Pinaki},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--12},
  year={2020},
  publisher={Nature Publishing Group},
  doi={10.1038/s41598-020-67880-z}
}


@article{GrazianiHenning:2020:ConceptAttribution,
   year = {2020},
   author = {Graziani, Mara and Andrearczyk, Vincent and Marchand-Maillet, Stéphane and Mueller, Henning},
   title = {Concept attribution: Explaining {CNN} decisions to physicians},
   journal = {Computers in biology and medicine},
   volume = {123},
   number = {8},
   pages = {103865},
   abstract = {Deep learning explainability is often reached by gradient-based approaches that attribute the network output to perturbations of the input pixels. However, the relevance of input pixels may be difficult to relate to relevant image features in some applications, e.g. diagnostic measures in medical imaging. The framework described in this paper shifts the attribution focus from pixel values to user-defined concepts. By checking if certain diagnostic measures are present in the learned representations, experts can explain and entrust the network output. Being post-hoc, our method does not alter the network training and can be easily plugged into the latest state-of-the-art convolutional networks. This paper presents the main components of the framework for attribution to concepts, in addition to the introduction of a spatial pooling operation on top of the feature maps to obtain a solid interpretability analysis. Furthermore, regularized regression is analyzed as a solution to the regression overfitting in high-dimensionality latent spaces. The versatility of the proposed approach is shown by experiments on two medical applications, namely histopathology and retinopathy, and on one non-medical task, the task of handwritten digit classification. The obtained explanations are in line with clinicians’ guidelines and complementary to widely used visualization tools such as saliency maps.},
   doi = {10.1016/j.compbiomed.2020.103865}
}


@article{GuidottiPedreschi:2019:Survey,
   year = {2019},
   author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
   title = {A survey of methods for explaining black box models},
   journal = {ACM computing surveys (CSUR)},
   volume = {51},
   number = {5},
   pages = {93},
   doi = {10.1145/3236009}
}

@article{gunning2019darpa,
  title={{DARPA}’s explainable artificial intelligence ({XAI}) program},
  author={Gunning, David and Aha, David},
  journal={AI Magazine},
  volume={40},
  number={2},
  pages={44--58},
  year={2019}
}


%%% HHH

@article{hamet2017artificial,
  title={Artificial intelligence in medicine},
  author={Hamet, Pavel and Tremblay, Johanne},
  journal={Metabolism},
  volume={69},
  pages={S36--S40},
  year={2017},
  publisher={Elsevier},
  doi = {10.1016/j.metabol.2017.01.011}
}


@inproceedings{holzinger2013human,
  title={Human-Computer Interaction and Knowledge Discovery (HCI-KDD): What is the benefit of bringing those two fields to work together?},
  author={Holzinger, Andreas},
  booktitle={International conference on availability, reliability, and security},
  pages={319--328},
  year={2013},
  organization={Springer}
}

@article{Holzinger:2016:iML,
   year = {2016},
   author = {Holzinger, Andreas},
   title = {Interactive Machine Learning for Health Informatics: When do we need the human-in-the-loop?},
   journal = {Brain Informatics},
   volume = {3},
   number = {2},
   pages = {119--131},
   abstract = {Machine learning (ML) is the fastest growing field in computer science, and health informatics is amongst the greatest challenges. The goal of ML is to develop algorithms which can learn and improve over time and can be used for predictions. Most ML researchers concentrate on automatic Machine Learning (aML), where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from big data with many training sets. However, in the health domain, sometimes we are confronted with a small number of data sets or rare events, where aML-approaches suffer of insufficient training samples. Here interactive Machine Learning (iML) may be of help, having its roots in Reinforcement Learning (RL), Preference Learning (PL) and Active Learning (AL). The term iML is not yet well used, so we define it as algorithms that can interact with agents and can optimize their learning behaviour through these interactions, where the agents can also be human. This human-in-the-loop can be beneficial in solving computationally hard problems, e.g., subspace clustering, protein folding, or k-anonymization of health data, where human expertise can help to reduce an exponential search space through heuristic selection of samples. Therefore, what would otherwise be an NP-hard problem, reduces greatly in complexity through the input and the assistance of a human agent involved in the learning phase.},
   keywords = {interactive Machine learning, health informatics},
   doi = {10.1007/s40708-016-0042-6}
}

@article{Holzinger:2011:acceptance,
   year = {2011},
   author = {Holzinger, Andreas and Searle, Gig and Wernbacher, Michaela},
   title = {The effect of Previous Exposure to Technology (PET) on Acceptance and its importance in Usability Engineering},
   journal = {Springer Universal Access in the Information Society International Journal},
   volume = {10},
   number = {3},
   pages = {245--260},
   doi = {10.1007/s10209-010-0212-x}
}

@article{holzinger_towards_2017,
	title = {Towards the {Augmented} {Pathologist}: {Challenges} of {Explainable}-{AI} in {Digital} {Pathology}},
	shorttitle = {Towards the {Augmented} {Pathologist}},
	url = {http://arxiv.org/abs/1712.06657},
	abstract = {Digital pathology is not only one of the most promising fields of diagnostic medicine, but at the same time a hot topic for fundamental research. Digital pathology is not just the transfer of histopathological slides into digital representations. The combination of different data sources (images, patient records, and *omics data) together with current advances in artificial intelligence/machine learning enable to make novel information accessible and quantifiable to a human expert, which is not yet available and not exploited in current medical settings. The grand goal is to reach a level of usable intelligence to understand the data in the context of an application task, thereby making machine decisions transparent, interpretable and explainable. The foundation of such an "augmented pathologist" needs an integrated approach: While machine learning algorithms require many thousands of training examples, a human expert is often confronted with only a few data points. Interestingly, humans can learn from such few examples and are able to instantly interpret complex patterns. Consequently, the grand goal is to combine the possibilities of artificial intelligence with human intelligence and to find a well-suited balance between them to enable what neither of them could do on their own. This can raise the quality of education, diagnosis, prognosis and prediction of cancer and other diseases. In this paper we describe some (incomplete) research issues which we believe should be addressed in an integrated and concerted effort for paving the way towards the augmented pathologist.},
	urldate = {2020-07-31},
	journal = {arXiv:1712.06657},
	author = {Holzinger, Andreas and Malle, Bernd and Kieseberg, Peter and Roth, Peter M. and Mueller, Heimo and Reihs, Robert and Zatloukal, Kurt},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.06657},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/theoevans/Zotero/storage/QRQ6UU4T/Holzinger et al. - 2017 - Towards the Augmented Pathologist Challenges of E.pdf:application/pdf;arXiv.org Snapshot:/Users/theoevans/Zotero/storage/K4XZ6R9I/1712.html:text/html}
}

@incollection{HolzingerEtAl:2017:DigitalPathologyMachineLearning,
   year = {2017},
   author = {Holzinger, Andreas and Malle, Bernd and Kieseberg, Peter and Roth, Peter M. and Mueller, Heimo and Reihs, Robert and Zatloukal, Kurt},
   title = {Machine Learning and Knowledge Extraction in Digital Pathology needs an integrative approach},
   booktitle = {Towards Integrative Machine Learning and Knowledge Extraction, Springer Lecture Notes in Artificial Intelligence Volume LNAI 10344},
   publisher = {Springer},
   address = {Cham},
   pages = {13--50},
   doi = {10.1007/978-3-319-69775-8_2}
}

@incollection{Holzinger:2018:IEEE-DISA,
   year = {2018},
   author = {Holzinger, Andreas},
   title = {From Machine Learning to Explainable {AI}},
   booktitle = {2018 World Symposium on Digital Intelligence for Systems and Machines (IEEE DISA)},
   publisher = {IEEE},
   pages = {55--66},
   abstract = {The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were comprehensible “glass-box” approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.},
   doi = {10.1109/DISA.2018.8490530}
}

@article{HolzingerEtAl:2019:Wiley-Paper,
   year = {2019},
   author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and Mueller, Heimo},
   title = {Causability and Explainability of Artificial Intelligence in Medicine},
   journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
   volume = {9},
   number = {4},
   pages = {1--13},
   abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black‐box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use‐case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system.},
   doi = {10.1002/widm.1312}
}

@article{Holzinger:2019:HumanLoopAPIN,
   year = {2019},
   author = {Holzinger, Andreas and Plass, Markus and Kickmeier-Rust, Michael and Holzinger, Katharina and Crisan, Gloria Cerasela and Pintea, Camelia-M. and Palade, Vasile},
   title = {Interactive machine learning: experimental evidence for the human in the algorithmic loop},
   journal = {Applied Intelligence},
   volume = {49},
   number = {7},
   pages = {2401--2414},
   abstract = {Recent advances in automatic machine learning (aML) allow solving problems without any human intervention. However, sometimes a human-in-the-loop can be beneficial in solving computationally hard problems. In this paper we provide new experimental insights on how we can improve computational intelligence by complementing it with human intelligence in an interactive machine learning approach (iML). For this purpose, we used the Ant Colony Optimization (ACO) framework, because this fosters multi-agent approaches with human agents in the loop. We propose unification between the human intelligence and interaction skills and the computational power of an artificial system. The ACO framework is used on a case study solving the Traveling Salesman Problem, because of its many practical implications, e.g. in the medical domain. We used ACO due to the fact that it is one of the best algorithms used in many applied intelligence problems. For the evaluation we used gamification, i.e. we implemented a snake-like game called Traveling Snakesman with the MAX–MIN Ant System (MMAS) in the background. We extended the MMAS–Algorithm in a way, that the human can directly interact and influence the ants. This is done by “traveling” with the snake across the graph. Each time the human travels over an ant, the current pheromone value of the edge is multiplied by 5. This manipulation has an impact on the ant’s behavior (the probability that this edge is taken by the ant increases). The results show that the humans performing one tour through the graphs have a significant impact on the shortest path found by the MMAS. Consequently, our experiment demonstrates that in our case human intelligence can positively influence machine intelligence. To the best of our knowledge this is the first study of this kind.},
   doi = {10.1007/s10489-018-1361-5}
}

@article{Holzinger:2020:explainable,
    author = {Andreas Holzinger},
    doi = {doi:10.1515/icom-2020-0024},
    url = {https://doi.org/10.1515/icom-2020-0024},
    title = {Explainable {AI} and Multi-Modal Causability in Medicine},
    journal = {i-com},
    number = {3},
    volume = {19},
    year = {2020},
    pages = {171--179}
}

@book{holzinger_artificial_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Artificial {Intelligence} and {Machine} {Learning} for {Digital} {Pathology}: {State}-of-the-{Art} and {Future} {Challenges}},
	author={Holzinger, Andreas and Goebel, Randy and Mengel, Michael and Mueller, Heimo},
	volume = {12090},
	isbn = {978-3-030-50401-4 978-3-030-50402-1},
	shorttitle = {Artificial {Intelligence} and {Machine} {Learning} for {Digital} {Pathology}},
	url = {http://link.springer.com/10.1007/978-3-030-50402-1},
	language = {en},
	urldate = {2020-08-05},
	publisher = {Springer International Publishing},
	year = {2020},
	doi = {10.1007/978-3-030-50402-1},
	file = {Holzinger et al. - 2020 - Artificial Intelligence and Machine Learning for D.pdf:/Users/theoevans/Zotero/storage/T42DPBRD/Holzinger et al. - 2020 - Artificial Intelligence and Machine Learning for D.pdf:application/pdf}
}

@article{HolzingerEtAl:2020:QualityOfExplanations,
   year = {2020},
   author = {Holzinger, Andreas and Carrington, Andre and Mueller, Heimo},
   title = {Measuring the Quality of Explanations: {The System Causability Scale (SCS)}. Comparing Human and Machine Explanations},
   journal = {KI - Kuenstliche Intelligenz (German Journal of Artificial intelligence), Special Issue on Interactive Machine Learning, Edited by Kristian Kersting, TU Darmstadt},
   volume = {34},
   number = {2},
   pages = {193--198},
   abstract = {Recent success in Artificial Intelligence (AI) and Machine Learning (ML) allow problem solving automatically without any human intervention. Autonomous approaches can be very convenient. However, in certain domains, e.g., in the medical domain, it is necessary to enable a domain expert to understand, why an algorithm came up with a certain result. Consequently, the field of Explainable {AI} (xAI) rapidly gained interest worldwide in various domains, particularly in medicine. Explainable {AI} studies transparency and traceability of opaque AI/ML and there are already a huge variety of methods. For example with layer-wise relevance propagation relevant parts of inputs to, and representations in, a neural network which caused a result, can be highlighted. This is a first important step to ensure that end users, e.g., medical professionals, assume responsibility for decision making with AI/ML and of interest to professionals and regulators. Interactive ML adds the component of human expertise to AI/ML processes by enabling them to re-enact and retrace AI/ML results, e.g. let them check it for plausibility. This requires new human-AI interfaces for explainable AI. In order to build effective and efficient interactive human-AI interfaces we have to deal with the question of how to evaluate the quality of explanations given by an explainable {AI} system. In this paper we introduce our System Causability Scale (SCS) to measure the quality of explanations. It is based on our notion of Causability (Holzinger et al., 2019) combined with concepts adapted from a widely accepted usability scale.},
   doi = {10.1007/s13218-020-00636-z}
}

@article{HolzingerEtAl:2021:GraphFusion,
   year = {2021},
   author = {Holzinger, Andreas and Malle, Bernd and Saranti, Anna and Pfeifer, Bastian},
   title = {Towards Multi-Modal Causability with Graph Neural Networks enabling Information Fusion for explainable {AI}},
   journal = {Information Fusion},
   volume = {71},
   number = {7},
   pages = {28--37},
   abstract = {AI is remarkably successful and outperforms human experts in certain tasks, even in complex domains such as medicine. Humans on the other hand are experts at multi-modal thinking and can embed new inputs almost instantly into a conceptual knowledge space shaped by experience. In many fields the aim is to build systems capable of explaining themselves, engaging in interactive what-if questions. Such questions, called counterfactuals, are becoming important in the rising field of explainable {AI} (xAI). Our central hypothesis is that using conceptual knowledge as a guiding model of reality will help to train more explainable, more robust and less biased machine learning models, ideally able to learn from fewer data. One important aspect in the medical domain is that various modalities contribute to one single result. Our main question is “How can we construct a multi-modal feature representation space (spanning images, text, genomics data) using knowledge bases as an initial connector for the development of novel explanation interface techniques?”. In this paper we argue for using Graph Neural Networks as a method-of-choice, enabling information fusion for multi-modal causability (causability – not to confuse with causality – is the measurable extent to which an explanation to a human expert achieves a specified level of causal understanding). The aim of this paper is to motivate the international xAI community to further work into the fields of multi-modal embeddings and interactive explainability, to lay the foundations for effective future human–AI interfaces. We emphasize that Graph Neural Networks play a major role for multi-modal causability, since causal links between features can be defined directly using graph structures.},
   doi = {10.1016/j.inffus.2021.01.008}
}

@misc{HolzingerEtAl:2021:PersonasToolbox,
   title={{Personas for {AI}: An Open Source Toolbox}},
   author={Holzinger, Andreas and Kargl, Michaela and Kipperer, Bettina and Regitnig, Peter and Plass, Markus and Mueller, Heimo},
   year={2021},
   url={https://github.com/human-centered-ai-lab/PERSONAS}
}
  

@article{homeyer2021artificial,
  title={Artificial intelligence in pathology: From prototype to product},
  author={Homeyer, Andr{\'e} and Lotz, Johannes and Schwen, Lars Ole and Weiss, Nick and Romberg, Daniel and H{\"o}fener, Henning and Zerbe, Norman and Hufnagl, Peter},
  journal={Journal of Pathology Informatics},
  volume={12},
  year={2021},
  publisher={Wolters Kluwer--Medknow Publications}
}

@article{HudecEtAl-2021-Interpretable,
   year = {2021},
   author = {Hudec, Miroslav and Minarikova, Erika and Mesiar, Radko and Saranti, Anna and Holzinger, Andreas},
   title = {Classification by ordinal sums of conjunctive and disjunctive functions for explainable {AI} and interpretable machine learning solutions},
   journal = {Knowledge Based Systems},
   volume = {220},
   pages = {106916},
   abstract = {The main goal of classification is dividing entities into several classes. The classification considering uncertainty of belonging to the classes separates entities into the classes yes, no, maybe, where it is desirable to indicate the inclination towards belonging to yes or no. Neural networks have proven their high performance in sharp classification, but the solution is not traceable and therefore difficult or impossible for a human expert to interpret and to understand. Rule-based systems are explainable in principle, however, are based on formal inference structures and also have problems with interpretability due to their high complexity. We must stress that even human experts sometimes cannot explain, but rather construct mental models of the problem and consult these models to select the best possible solution. In our work, we propose classification by aggregation functions of the mixed behaviour by the variability in ordinal sums of conjunctive and disjunctive functions. In this way, domain experts should only assign the key observations regarding considered attributes. Consequently, the variability of functions provides room for machine learning to learn the best possible option from data. Such a solution is re-traceable, reproducible, and explainable to domain experts. In this paper we discuss the proposed approach on examples and outline the research steps in interactive machine learning with a human-in-the-loop via aggregation functions.},
   doi = {10.1016/j.knosys.2021.106916}
}


@book{Hunt:1962:conceptLearning,
   year = {1962},
   author = {Hunt, Earl B.},
   title = {Concept learning: An information processing problem},
   publisher = {John Wiley and Sons},
   address = {Hoboken (NJ)},
   abstract = {The ability to think in terms of abstractions is one of the most powerful tools man possesses. It is literally true that we never step into the same river twice; every situation is in some sense unique. Yet we manage to order our experience into coherent categories by defining a given situation as a member of that collection of situations for which responses x, y, etc. are appropriate. We classify. Classification is not a passive process. Tests must be made to determine whether the present situation contains certain elements or whether it can be described in a particular way. The results of these tests provide the information we use to guide the classifying act. But how do we develop rules for testing? This is the question to which the present research has been addressed. Understanding how humans learn abstractions is essential to the understanding of human thought. This monograph has been organized with a definite view in mind. We wished to present a unified picture of current research and thought on the topic of concept learning. It was felt that workers in several disciplines, proceeding quite independently of each other, have made substantial contributions to the field. The psychological study of concept learning has undergone a resurgence in the past few years. At the same time, workers interested in the design of artificial intelligence systems have faced the problem of how concepts ought to be learned. A major purpose of this text is to present a synthesis of the work in these separate but related areas. Concepts are essentially definitions in symbolic logic. Therefore, their role in logic should be considered. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
   doi = {http://dx.doi.org/10.1037/13135-001},

}



%%% KKK


@incollection{Kargl-et-al:2020:PathoWorkflows,
   year = {2020},
   author = {Kargl, Michaela and Regitnig, Peter and Mueller, Heimo and Holzinger, Andreas},
   title = {Towards a Better Understanding of the Workflows: Modeling Pathology Processes in View of Future {AI} Integration},
   booktitle = {Springer Lecture Notes in Artificial Intelligence LNAI 12090},
   publisher = {Springer/Nature},
   address = {Cham},
   pages = {102--117},
   doi = {10.1007/978-3-030-50402-1-7}
}


@article{KadirBrady:2001:Saliency,
   year = {2001},
   author = {Kadir, Timor and Brady, Michael},
   title = {Saliency, scale and image description},
   journal = {International Journal of Computer Vision},
   volume = {45},
   number = {2},
   pages = {83--105},
   doi = {10.1023/A:1012460413855}
}



@article{klauschen2015standardized,
  title={Standardized Ki67 diagnostics using automated scoring—clinical validation in the GeparTrio breast cancer study},
  author={Klauschen, Frederick and Wienert, Stephan and Schmitt, Wolfgang D and Loibl, Sibylle and Gerber, Bernd and Blohmer, Jens-Uwe and Huober, Jens and R{\"u}diger, Thomas and Erbst{\"o}{\ss}er, Erhard and Mehta, Keyur and others},
  journal={Clinical Cancer Research},
  volume={21},
  number={16},
  pages={3651--3657},
  year={2015},
  publisher={AACR}
}


%%% LLL


@inproceedings{LakkarajuLeskovec:2019:trust,
   year = {2019},
   author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
   title = {Faithful and customizable explanations of black box models},
   pages = {131--138},
   booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (AIES '19)}
}




@inproceedings{lakshmi2020deep,
  title={Deep learning model based {Ki-67} index estimation with automatically labelled data},
  author={Lakshmi, S and Ritwik, Kotra Venkata Sai and Vijayasenan, Deepu and Sreeram, Saraswathy and Suresh, Pooja K and others},
  booktitle={2020 42nd Annual International Conference of the IEEE Engineering in Medicine \& Biology Society (EMBC)},
  pages={1412--1415},
  year={2020},
  organization={IEEE},
  doi={10.1109/EMBC44109.2020.9175752}
}

@article{LapuschkinEtAl:2016:LRP,
   year = {2016},
   author = {Lapuschkin, Sebastian and Binder, Alexander and Montavon, Gregoire and Mueller, Klaus-Robert and Samek, Wojciech},
   title = {The LRP toolbox for artificial neural networks},
   journal = {The Journal of Machine Learning Research (JMLR)},
   volume = {17},
   number = {1},
   pages = {3938--3942}
}

@incollection{LeCun:1988:BackProp,
   year = {1988},
   author = {LeCun, Yann},
   title = {A theoretical framework for back-propagation},
   booktitle = {Proceedings of the 1988 connectionist models summer school},
   editor = {Touresky, D. and Hinton, G. and Sejnowski, T. J.},
   publisher = {Morgan Kaufmann},
   address = {Pittsburgh (PA)},
   volume = {1},
   pages = {21--28}
}

@article{LeCunBengioHinton:2015:DeepLearningNature,
   year = {2015},
   author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
   title = {Deep learning},
   journal = {Nature},
   volume = {521},
   number = {7553},
   pages = {436-444},
   doi = {10.1038/nature14539}
}


@article{Longoni:2019:resistance,
   year = {2019},
   author = {Longoni, Chiara and Bonezzi, Andrea and Morewedge, Carey K.},
   title = {Resistance to medical artificial intelligence},
   journal = {Journal of Consumer Research},
   volume = {46},
   number = {4},
   pages = {629--650},
   doi = {10.1093/jcr/ucz013}
}


@inbook{liao2020questioning,
author = {Liao, Q. Vera and Gruen, Daniel and Miller, Sarah},
title = {Questioning the {AI}: Informing Design Practices for Explainable {AI} User Experiences},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376590},
abstract = {A surge of interest in explainable {AI} (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in {AI} systems, how to address real-world user needs for understanding {AI} remains an open question. By interviewing 20 UX and design practitioners working on various {AI} products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable {AI} products. To do so, we develop an algorithm-informed
XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–15},
numpages = {15}
}


%%% MMM

@inproceedings{MorchEtAl:1995:Saliency,
   year = {1995},
   author = {Morch, Niels J. S. and Kjems, Ulrik and Hansen, Lars Kai and Svarer, Claus and Law, Ian and Lautrup, Benny and Strother, Steve and Rehm, Kelly},
   title = {Visualization of neural networks using saliency maps},
   booktitle = {Proceedings of ICNN'95-International Conference on Neural Networks},
   publisher = {IEEE},
   pages = {2085-2090},
   doi = {10.1109/ICNN.1995.488997}
}



@article{MuellerEtAl:2021:TenCommandments,
   year = {2021},
   author = {Mueller, Heimo and Mayrhofer, Michaela T. and Veen, Evert-Ben Van and Holzinger, Andreas},
   title = {{The Ten Commandments of Ethical Medical AI}},
   journal = {IEEE COMPUTER},
   volume = {54},
   number = {7},
   pages = {119--123},
   doi = {10.1109/MC.2021.3074263}
}


%%% NNN

@article{nadler2013ki,
  title={{Ki-67} is a reliable pathological grading marker for neuroendocrine tumors},
  author={Nadler, Ashlie and Cukier, Moises and Rowsell, Corwyn and Kamali, Sepideh and Feinberg, Yael and Singh, Simron and Law, Calvin HL},
  journal={Virchows archiv},
  volume={462},
  number={5},
  pages={501--505},
  year={2013},
  publisher={Springer}
}

@article{negahbani2021pathonet,
  title={{PathoNet} introduced as a deep neural network backend for evaluation of {Ki-67} and tumor-infiltrating lymphocytes in breast cancer},
  author={Negahbani, Farzin and Sabzi, Rasool and Jahromi, Bita Pakniyat and Firouzabadi, Dena and Movahedi, Fateme and Shirazi, Mahsa Kohandel and Majidi, Shayan and Dehghanian, Amirreza},
  journal={Scientific reports},
  volume={11},
  number={1},
  pages={1--13},
  year={2021},
  publisher={Nature Publishing Group}
}

@misc{nielsen2005ten,
  title={Ten usability heuristics},
  author={Nielsen, Jakob},
  year={2005},
  publisher={http://www. nngroup. com/articles/ten-usability-heuristics/(acc-essed~…}
}


%%%PPP

@article{Pantanowitz:2010:DigitalPathology,
   year = {2010},
   author = {Pantanowitz, Liron},
   title = {Digital images and the future of digital pathology},
   journal = {Journal of pathology informatics},
   volume = {1},
   number = {15},
   pages = {1-4},
   doi = {10.4103/2153-3539.68332}
}

@article{PantanowitzEtAl:2021:AIPatho,
   year = {2021},
   author = {Pantanowitz, Liron and Wu, Uno and Seigh, Lindsey and LoPresti, Edmund and Yeh, Fang-Cheng and Salgia, Payal and Michelow, Pamela and Hazelhurst, Scott and Chen, Wei-Yu and Hartman, Douglas},
   title = {Artificial Intelligence–Based Screening for Mycobacteria in Whole-Slide Images of Tissue Samples},
   journal = {American Journal of Clinical Pathology},
   volume = {156},
   number = {1},
   pages = {117--128},
   doi = {https://doi.org/10.1093/ajcp/aqaa215}
}

@article{Pearl:2019:CACM,
   year = {2019},
   author = {Pearl, Judea},
   title = {The seven tools of causal inference, with reflections on machine learning},
   journal = {Communications of the ACM},
   volume = {62},
   number = {3},
   pages = {54-60}
}

@article{piccialli_artificial_2021,
	title = {Artificial intelligence and healthcare: {Forecasting} of medical bookings through multi-source time-series fusion},
	volume = {74},
	issn = {1566-2535},
	shorttitle = {Artificial intelligence and healthcare},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253521000592},
	doi = {10.1016/j.inffus.2021.03.004},
	abstract = {Nowadays, Artificial intelligence (AI), combined with the digitalization of healthcare, can lead to substantial improvements in Patient Care, Disease Management, Hospital Administration, and supply chain effectiveness. Among predictive analytics tools, time series forecasting represents a central task to support healthcare management in terms of bookings and medical services predictions. In this context, the development of flexible frameworks to provide robust and reliable predictions became a central point in this healthcare innovation process. This paper presents and discusses a multi-source time series fusion and forecasting framework relying on Deep Learning. By combining weather, air-quality and medical bookings time series through a feature compression stage which preserves temporal patterns, the prediction is provided through a flexible ensemble technique based on machine learning models and a hybrid neural network. The proposed system is able to predict the number of bookings related to a specific medical examination for a 7-days horizon period. To assess the proposed approach’s effectiveness, we rely on time series extracted from a real dataset of administrative e-health records provided by the Campania Region health department, in Italy.},
	language = {en},
	urldate = {2021-04-22},
	journal = {Information Fusion},
	author = {Piccialli, Francesco and Giampaolo, Fabio and Prezioso, Edoardo and Camacho, David and Acampora, Giovanni},
	month = oct,
	year = {2021},
	keywords = {Artificial intelligence, Deep Learning, Healthcare, Multi-source time-series},
	pages = {1--16},
	file = {ScienceDirect Snapshot:/Users/theoevans/Zotero/storage/PIGNLZ5Y/S1566253521000592.html:text/html}
}

@article{piccialli_survey_2021,
	title = {A survey on deep learning in medicine: {Why}, how and when?},
	volume = {66},
	issn = {1566-2535},
	shorttitle = {A survey on deep learning in medicine},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253520303651},
	doi = {10.1016/j.inffus.2020.09.006},
	abstract = {New technologies are transforming medicine, and this revolution starts with data. Health data, clinical images, genome sequences, data on prescribed therapies and results obtained, data that each of us has helped to create. Although the first uses of artificial intelligence (AI) in medicine date back to the 1980s, it is only with the beginning of the new millennium that there has been an explosion of interest in this sector worldwide. We are therefore witnessing the exponential growth of health-related information with the result that traditional analysis techniques are not suitable for satisfactorily management of this vast amount of data. {AI} applications (especially Deep Learning), on the other hand, are naturally predisposed to cope with this explosion of data, as they always work better as the amount of training data increases, a phase necessary to build the optimal neural network for a given clinical problem. This paper proposes a comprehensive and in-depth study of Deep Learning methodologies and applications in medicine. An in-depth analysis of the literature is presented; how, where and why Deep Learning models are applied in medicine are discussed and reviewed. Finally, current challenges and future research directions are outlined and analysed.},
	language = {en},
	urldate = {2021-04-22},
	journal = {Information Fusion},
	author = {Piccialli, Francesco and Somma, Vittorio Di and Giampaolo, Fabio and Cuomo, Salvatore and Fortino, Giancarlo},
	month = {feb},
	year = {2021},
	keywords = {Artificial intelligence, Data science, Deep learning, Medicine, Neural networks},
	pages = {111--137},
	file = {ScienceDirect Snapshot:/Users/theoevans/Zotero/storage/SU87876P/S1566253520303651.html:text/html}
}

@incollection{poceviciute_survey_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Survey of {XAI} in {Digital} {Pathology}},
	isbn = {978-3-030-50402-1},
	url = {https://doi.org/10.1007/978-3-030-50402-1_4},
	abstract = {Artificial intelligence (AI) has shown great promise for diagnostic imaging assessments. However, the application of {AI} to support medical diagnostics in clinical routine comes with many challenges. The algorithms should have high prediction accuracy but also be transparent, understandable and reliable. Thus, explainable artificial intelligence (XAI) is highly relevant for this domain. We present a survey on XAI within digital pathology, a medical imaging sub-discipline with particular characteristics and needs. The review includes several contributions. Firstly, we give a thorough overview of current XAI techniques of potential relevance for deep learning methods in pathology imaging, and categorise them from three different aspects. In doing so, we incorporate uncertainty estimation methods as an integral part of the XAI landscape. We also connect the technical methods to the specific prerequisites in digital pathology and present findings to guide future research efforts. The survey is intended for both technical researchers and medical professionals, one of the objectives being to establish a common ground for cross-disciplinary discussions.},
	language = {en},
	urldate = {2020-08-05},
	booktitle = {Artificial {Intelligence} and {Machine} {Learning} for {Digital} {Pathology}: {State}-of-the-{Art} and {Future} {Challenges}},
	publisher = {Springer International Publishing},
	author = {Pocevičiūtė, Milda and Eilertsen, Gabriel and Lundström, Claes},
	editor = {Holzinger, Andreas and Goebel, Randy and Mengel, Michael and Mueller, Heimo},
	year = {2020},
	doi = {10.1007/978-3-030-50402-1_4},
	keywords = {Digital pathology, AI, Medical imaging, XAI},
	pages = {56--88},
	file = {Springer Full Text PDF:/Users/theoevans/Zotero/storage/TD83KLSG/Pocevičiūtė et al. - 2020 - Survey of XAI in Digital Pathology.pdf:application/pdf}
}

@article{polley2015international,
  title={An international study to increase concordance in Ki67 scoring},
  author={Polley, Mei-Yin C and Leung, Samuel CY and Gao, Dongxia and Mastropasqua, Mauro G and Zabaglo, Lila A and Bartlett, John MS and McShane, Lisa M and Enos, Rebecca A and Badve, Sunil S and Bane, Anita L and others},
  journal={Modern pathology},
  volume={28},
  number={6},
  pages={778--786},
  year={2015},
  publisher={Nature Publishing Group},
  doi={10.1038/modpathol.2015.38}
}


@article{ProsperiEtAl:2020:CausalHealth,
   year = {2020},
   author = {Prosperi, Mattia and Guo, Yi and Sperrin, Matt and Koopman, James S. and Min, Jae S. and He, Xing and Rich, Shannan and Wang, Mo and Buchan, Iain E. and Bian, Jiang},
   title = {Causal inference and counterfactual prediction in machine learning for actionable healthcare},
   journal = {Nature Machine Intelligence},
   volume = {2},
   number = {7},
   pages = {369--375},
   doi = {10.1038/s42256-020-0197-y}
}

%%% QQQ

@article{quinn:trustmedicalai:2020,
    author = {Quinn, Thomas P and Senadeera, Manisha and Jacobs, Stephan and Coghlan, Simon and Le, Vuong},
    title = "{Trust and medical {AI}: the challenges we face and the expertise needed to overcome them}",
    journal = {Journal of the American Medical Informatics Association},
    volume = {28},
    number = {4},
    pages = {890-894},
    year = {2020},
    month = {12},
    abstract = "{Artificial intelligence (AI) is increasingly of tremendous interest in the medical field. How-ever, failures of medical {AI} could have serious consequences for both clinical outcomes and the patient experience. These consequences could erode public trust in AI, which could in turn undermine trust in our healthcare institutions. This article makes 2 contributions. First, it describes the major conceptual, technical, and humanistic challenges in medical AI. Second, it proposes a solution that hinges on the education and accreditation of new expert groups who specialize in the development, verification, and operation of medical {AI} technologies. These groups will be required to maintain trust in our healthcare institutions.}",
    issn = {1527-974X},
    doi = {10.1093/jamia/ocaa268},
    url = {https://doi.org/10.1093/jamia/ocaa268},
    eprint = {https://academic.oup.com/jamia/article-pdf/28/4/890/36642134/ocaa268.pdf},
}



%%% RRR

@incollection{regitnig_expectations_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Expectations of {Artificial} {Intelligence} for {Pathology}},
	isbn = {978-3-030-50402-1},
	url = {https://doi.org/10.1007/978-3-030-50402-1_1},
	abstract = {Within the last ten years, essential steps have been made to bring artificial intelligence (AI) successfully into the field of pathology. However, most pathologists are still far away from using {AI} in daily pathology practice. If one leaves the pathology annihilation model, this paper focuses on tasks, which could be solved, and which could be done better by AI, or image-based algorithms, compared to a human expert. In particular, this paper focuses on the needs and demands of surgical pathologists; examples include: Finding small tumour deposits within lymph nodes, detection and grading of cancer, quantification of positive tumour cells in immunohistochemistry, pre-check of Papanicolaou-stained gynaecological cytology in cervical cancer screening, text feature extraction, text interpretation for tumour-coding error prevention and {AI} in the next-generation virtual autopsy. However, in order to make substantial progress in both fields it is important to intensify the cooperation between medical {AI} experts and pathologists.},
	language = {en},
	urldate = {2020-08-11},
	booktitle = {Artificial {Intelligence} and {Machine} {Learning} for {Digital} {Pathology}: {State}-of-the-{Art} and {Future} {Challenges}},
	publisher = {Springer International Publishing},
	author = {Regitnig, Peter and Mueller, Heimo and Holzinger, Andreas},
	editor = {Holzinger, Andreas and Goebel, Randy and Mengel, Michael and Mueller, Heimo},
	year = {2020},
	doi = {10.1007/978-3-030-50402-1_1},
	keywords = {Machine learning, Digital pathology, Artificial Intelligence, Medical AI, Practical implementations of AI},
	pages = {1--15},
	file = {Springer Full Text PDF:/Users/theoevans/Zotero/storage/8IVFNTBJ/Regitnig et al. - 2020 - Expectations of Artificial Intelligence for Pathol.pdf:application/pdf}
}

@article{rimm2019international,
  title={An international multicenter study to evaluate reproducibility of automated scoring for assessment of Ki67 in breast cancer},
  author={Rimm, David L and Leung, Samuel CY and McShane, Lisa M and Bai, Yalai and Bane, Anita L and Bartlett, John MS and Bayani, Jane and Chang, Martin C and Dean, Michelle and Denkert, Carsten and others},
  journal={Modern Pathology},
  volume={32},
  number={1},
  pages={59--69},
  year={2019},
  publisher={Nature Publishing Group},
  doi={10.1038/s41379-018-0109-4}
}

@article{Rudin:2019:interpretable,
   year = {2019},
   author = {Rudin, Cynthia},
   title = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
   journal = {Nature Machine Intelligence},
   volume = {1},
   number = {5},
   pages = {206-215},
   doi = {10.1038/s42256-019-0048-x}
}

@article{runeson_guidelines_2008,
	title = {Guidelines for conducting and reporting case study research in software engineering},
	volume = {14},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-008-9102-8},
	doi = {10.1007/s10664-008-9102-8},
	abstract = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors’ own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.},
	language = {en},
	number = {2},
	urldate = {2021-06-14},
	journal = {Empirical Software Engineering},
	author = {Runeson, Per and Höst, Martin},
	month = dec,
	year = {2008},
	pages = {131},
	file = {Springer Full Text PDF:files/868/Runeson and Höst - 2008 - Guidelines for conducting and reporting case study.pdf:application/pdf}
}


%%% SSS
@inproceedings{SimonyanVedaldiZisserman:2013:DeepInside,
  title={Deep inside convolutional networks: Visualising image classification models and saliency maps},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={In Workshop at International Conference on Learning Representations},
  year={2014},
  organization={Citeseer}
}

@article{Simpson:2007:trust,
   year = {2007},
   author = {Simpson, Jeffry A.},
   title = {Psychological foundations of trust},
   journal = {Current directions in psychological science},
   volume = {16},
   number = {5},
   pages = {264-268},
   abstract = {Trust lies at the foundation of nearly all major theories of interpersonal relationships. Despite its great theoretical importance, a limited amount of research has examined how and why trust develops, is maintained, and occasionally unravels in relationships. Following a brief overview of theoretical and empirical milestones in the interpersonal-trust literature, an integrative process model of trust in dyadic relationships is presented.},
   doi = {10.1111/j.1467-8721.2007.00517.x}
}



@article{samek2017explainable,
  title={Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models},
  author={Samek, Wojciech and Wiegand, Thomas and M{\"u}ller, Klaus-Robert},
  journal={arXiv preprint arXiv:1708.08296},
  year={2017}
}

@incollection{Schneeberger:2020:legalAI,
   year = {2020},
   author = {Schneeberger, David and Stoeger, Karl and Holzinger, Andreas},
   title = {The {European} legal framework for medical {AI}},
   booktitle = {International Cross-Domain Conference for Machine Learning and Knowledge Extraction, Springer LNCS 12279},
   publisher = {Springer},
   address = {Cham},
   pages = {209--226},
   doi = {10.1007/978-3-030-57321-8-12}
}

@article{schorr_neuroscope_2021,
	title = {Neuroscope: {An} {Explainable} {AI} {Toolbox} for {Semantic} {Segmentation} and {Image} {Classification} of {Convolutional} {Neural} {Nets}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Neuroscope},
	url = {https://www.mdpi.com/2076-3417/11/5/2199},
	doi = {10.3390/app11052199},
	abstract = {Trust in artificial intelligence (AI) predictions is a crucial point for a widespread acceptance of new technologies, especially in sensitive areas like autonomous driving. The need for tools explaining {AI} for deep learning of images is thus eminent. Our proposed toolbox Neuroscope addresses this demand by offering state-of-the-art visualization algorithms for image classification and newly adapted methods for semantic segmentation of convolutional neural nets (CNNs). With its easy to use graphical user interface (GUI), it provides visualization on all layers of a CNN. Due to its open model-view-controller architecture, networks generated and trained with Keras and PyTorch are processable, with an interface allowing extension to additional frameworks. We demonstrate the explanation abilities provided by Neuroscope using the example of traffic scene analysis.},
	language = {en},
	number = {5},
	urldate = {2021-05-21},
	journal = {Applied Sciences},
	author = {Schorr, Christian and Goodarzi, Payman and Chen, Fei and Dahmen, Tim},
	month = jan,
	year = {2021},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {explainable AI, image classification, convolutional neural nets, semantic segmentation},
	pages = {2199},
	file = {Full Text PDF:files/851/Schorr et al. - 2021 - Neuroscope An Explainable {AI} Toolbox for Semantic.pdf:application/pdf;Snapshot:files/852/2199.html:text/html}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}

@incollection{SinghEtAl:2017:RNN-AAL,
   year = {2017},
   author = {Singh, Deepika and Merdivan, Erinc and Psychoula, Ismini and Kropf, Johannes and Hanke, Sten and Geist, Matthieu and Holzinger, Andreas},
   title = {Human Activity Recognition Using Recurrent Neural Networks},
   booktitle = {Machine Learning and Knowledge Extraction, CD-MAKE, Lecture Notes in Computer Science LNCS 10410},
   publisher = {Springer International},
   address = {Cham},
   pages = {267--274},
   abstract = {Human activity recognition using smart home sensors is one of the bases of ubiquitous computing in smart environments and a topic undergoing intense research in the field of ambient assisted living. The increasingly large amount of data sets calls for machine learning methods. In this paper, we introduce a deep learning model that learns to classify human activities without using any prior knowledge. For this purpose, a Long Short Term Memory (LSTM) Recurrent Neural Network was applied to three real world smart home datasets. The results of these experiments show that the proposed approach outperforms the existing ones in terms of accuracy and performance.},
   doi = {10.1007/978-3-319-66808-6-8}
}

@article{singh_explainable_2020,
	title = {Explainable deep learning models in medical image analysis},
	url = {http://arxiv.org/abs/2005.13799},
	abstract = {Deep learning methods have been very effective for a variety of medical diagnostic tasks and has even beaten human experts on some of those. However, the black-box nature of the algorithms has restricted clinical use. Recent explainability studies aim to show the features that influence the decision of a model the most. The majority of literature reviews of this area have focused on taxonomy, ethics, and the need for explanations. A review of the current applications of explainable deep learning for different medical imaging tasks is presented here. The various approaches, challenges for clinical deployment, and the areas requiring further research are discussed here from a practical standpoint of a deep learning researcher designing a system for the clinical end-users.},
	urldate = {2021-05-27},
	journal = {arXiv:2005.13799 [cs, eess]},
	author = {Singh, Amitojdeep and Sengupta, Sourya and Lakshminarayanan, Vasudevan},
	month = may,
	year = {2020},
	note = {arXiv: 2005.13799},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: Preprint submitted to J.Imaging, MDPI},
	file = {arXiv Fulltext PDF:files/863/Singh et al. - 2020 - Explainable deep learning models in medical image .pdf:application/pdf;arXiv.org Snapshot:files/864/2005.html:text/html}
}


@article{stacke_measuring_2021,
	title = {Measuring {Domain} {Shift} for {Deep} {Learning} in {Histopathology}},
	volume = {25},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2020.3032060},
	abstract = {The high capacity of neural networks allows fitting models to data with high precision, but makes generalization to unseen data a challenge. If a domain shift exists, i.e. differences in image statistics between training and test data, care needs to be taken to ensure reliable deployment in real-world scenarios. In digital pathology, domain shift can be manifested in differences between whole-slide images, introduced by for example differences in acquisition pipeline - between medical centers or over time. In order to harness the great potential presented by deep learning in histopathology, and ensure consistent model behavior, we need a deeper understanding of domain shift and its consequences, such that a model's predictions on new data can be trusted. This work focuses on the internal representation learned by trained convolutional neural networks, and shows how this can be used to formulate a novel measure - the representation shift - for quantifying the magnitude of model-specific domain shift. We perform a study on domain shift in tumor classification of hematoxylin and eosin stained images, by considering different datasets, models, and techniques for preparing data in order to reduce the domain shift. The results show how the proposed measure has a high correlation with drop in performance when testing a model across a large number of different types of domain shifts, and how it improves on existing techniques for measuring data shift and uncertainty. The proposed measure can reveal how sensitive a model is to domain variations, and can be used to detect new data that a model will have problems generalizing to. We see techniques for measuring, understanding and overcoming the domain shift as a crucial step towards reliable use of deep learning in the future clinical pathology applications.},
	number = {2},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Stacke, Karin and Eilertsen, Gabriel and Unger, Jonas and Lundström, Claes},
	month = feb,
	year = {2021},
	keywords = {Biomedical imaging, Biomedical measurement, Data models, Deep learning, Feature extraction, Histopathology, image classification, Image color analysis, learning (artificial intelligence), machine learning, Neural networks, pathology, unsupervised learning},
	pages = {325--336},
	file = {IEEE Xplore Abstract Record:/Users/theoevans/Zotero/storage/YT2U9QJM/9234592.html:text/html;Submitted Version:/Users/theoevans/Zotero/storage/BFW5CMH6/Stacke et al. - 2021 - Measuring Domain Shift for Deep Learning in Histop.pdf:application/pdf},
}


%%% TTT

@article{tjoa_survey_2020,
	title = {A {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI}): {Toward} {Medical} {XAI}},
	issn = {2162-2388},
	shorttitle = {A {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	doi = {10.1109/TNNLS.2020.3027314},
	abstract = {Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Tjoa, E. and Guan, C.},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {interpretability, Visualization, Biomedical imaging, Prediction algorithms, Artificial neural networks, Explainable artificial intelligence (XAI), Heating systems, Learning systems, machine learning (ML), medical information system, Reliability, survey.},
	pages = {1--21},
	file = {IEEE Xplore Abstract Record:/Users/theoevans/Zotero/storage/VUA2GC6J/9233366.html:text/html;IEEE Xplore Full Text PDF:/Users/theoevans/Zotero/storage/YFBV76SS/Tjoa and Guan - 2020 - A Survey on Explainable Artificial Intelligence (X.pdf:application/pdf}
}

@article{Topol:2019:NatureMedicine,
   year = {2019},
   author = {Topol, Eric J.},
   title = {High-performance medicine: the convergence of human and artificial intelligence},
   journal = {Nature medicine},
   volume = {25},
   number = {1},
   pages = {44-56},
   doi = {10.1038/s41591-018-0300-7}
}

@incollection{tosun_histomapr_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{HistoMapr}™: {An} {Explainable} {AI} ({xAI}) {Platform} for {Computational} {Pathology} {Solutions}},
	isbn = {978-3-030-50402-1},
	shorttitle = {{HistoMapr}™},
	url = {https://doi.org/10.1007/978-3-030-50402-1_13},
	abstract = {Pathologists are adopting whole slide images (WSIs) for diagnostic purposes. While doing so, pathologists should have all the information needed to make best diagnoses rapidly, while supervising computational pathology tools in real-time. Computational pathology has great potential for augmenting pathologists’ accuracy and efficiency, but concern exists regarding trust for ‘black-box AI’ solutions. Explainable {AI} (xAI) can reveal underlying reasons for its results, to promote safety, reliability, and accountability for critical tasks such as pathology diagnosis. Built on a hierarchy of computational and traditional image analysis algorithms, we present the development of our proprietary xAI software platform, HistoMapr, for pathologists to improve their efficiency and accuracy when viewing WSIs. HistoMapr and xAI represent a powerful and transparent alternative to ‘black-box’ AI. HistoMapr previews WSIs then presents key diagnostic areas first in an interactive, explainable fashion. Pathologists can access xAI features via a “Why?” button in the interface. Furthermore, two critical early application examples are presented: 1) Intelligent triaging that involves xAI estimation of difficulty for new cases to be forwarded to subspecialists or generalist pathologists; 2) Retrospective quality assurance entails detection of potential discrepancies between finalized results and xAI reviews. Finally, a prototype is presented for atypical ductal hyperplasia, a diagnostic challenge in breast pathology, where xAI descriptions were based on computational pipeline image results.},
	language = {en},
	urldate = {2020-08-05},
	booktitle = {Artificial {Intelligence} and {Machine} {Learning} for {Digital} {Pathology}: {State}-of-the-{Art} and {Future} {Challenges}},
	publisher = {Springer International Publishing},
	author = {Tosun, Akif Burak and Pullara, Filippo and Becich, Michael J. and Taylor, D. Lansing and Chennubhotla, S. Chakra and Fine, Jeffrey L.},
	editor = {Holzinger, Andreas and Goebel, Randy and Mengel, Michael and Mueller, Heimo},
	year = {2020},
	doi = {10.1007/978-3-030-50402-1_13},
	keywords = {Artificial Intelligence (AI), Breast pathology, Computational pathology, Computer assisted diagnosis, Digital pathology, Explainable {AI} (xAI)},
	pages = {204--227},
	file = {Springer Full Text PDF:files/302/Tosun et al. - 2020 - HistoMapr™ An Explainable {AI} (xAI) Platform for C.pdf:application/pdf}
}

@article{TranEtAl:2020:Recommender,
   year = {2020},
   author = {Tran, Thi Ngoc Trang and Felfernig, Alexander and Trattner, Christoph and Holzinger, Andreas},
   title = {Recommender systems in the healthcare domain: state-of-the-art and research issues},
   journal = {Journal of Intelligent Information Systems: Integrating Artificial Intelligence and Database Technologies},
   volume = {56},
   pages = {1-31},
   doi = {10.1007/s10844-020-00633-6}
}

%%% VVV

@article{Venkatesh:2003:acceptance,
   year = {2003},
   author = {Venkatesh, Viswanath and Morris, Michael G. and Davis, Gordon B. and Davis, Fred D.},
   title = {User acceptance of information technology: Toward a unified view},
   journal = {MIS quarterly},
   volume = {27},
   number = {3},
   pages = {425-478},
   doi = {10.2307/30036540 }
}




%%% WWW

@inproceedings{wang_designing_2019,
	address = {Glasgow Scotland Uk},
	title = {Designing {Theory}-{Driven} {User}-{Centric} {Explainable} {AI}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300831},
	doi = {10.1145/3290605.3300831},
	abstract = {From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable {AI} (XAI). This paper seeks to strengthen empirical applicationspecific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building humancentered, decision-theory-driven XAI based on an extensive review across these fields. Drawing on this framework, we identify pathways along which human cognitive patterns drives needs for building XAI and how XAI can mitigate common cognitive biases. We then put this framework into practice by designing and implementing an explainable clinical diagnostic tool for intensive care phenotyping and conducting a co-design exercise with clinicians. Thereafter, we draw insights into how this framework bridges algorithm-generated explanations and human decision-making theories. Finally, we discuss implications for XAI design and development.},
	language = {en},
	urldate = {2021-05-14},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y.},
	month = may,
	year = {2019},
	pages = {1--15},
	file = {Wang et al. - 2019 - Designing Theory-Driven User-Centric Explainable A.pdf:files/808/Wang et al. - 2019 - Designing Theory-Driven User-Centric Explainable A.pdf:application/pdf}
}

@article{WulczynEtAl:2021:AImed-example,
   year = {2021},
   author = {Wulczyn, Ellery},
   title = {Predicting prostate cancer specific-mortality with artificial intelligence-based Gleason grading},
   journal = {Communications Medicine},
   volume = {1},
   number = {1},
   pages = {1-8},
   doi = {https://doi.org/10.1038/s43856-021-00005-3}
}


%%% XXX

@inproceedings{xie2020chexplain,
  title={CheXplain: Enabling Physicians to Explore and Understand Data-Driven, {AI}-Enabled Medical Imaging Analysis},
  author={Xie, Yao and Chen, Melody and Kao, David and Gao, Ge and Chen, Xiang'Anthony'},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2020}
}


%%% ZZZ


@inproceedings{ZhuEtAl:2018:xAIDesigner,
   year = {2018},
   author = {Zhu, Jichen and Liapis, Antonios and Risi, Sebastian and Bidarra, Rafael and Youngblood, G. Michael},
   title = {Explainable {AI} for designers: A human-centered perspective on mixed-initiative co-creation},
   booktitle = {2018 IEEE Conference on Computational Intelligence and Games (CIG)},
   publisher = {IEEE},
   abstract = {Growing interest in eXplainable Artificial Intelligence (XAI) aims to make {AI} and machine learning more understandable to human users. However, most existing work focuses on new algorithms, and not on usability, practical interpretability and efficacy on real users. In this vision paper, we propose a new research area of eXplainable {AI} for Designers (XAID), specifically for game designers. By focusing on a specific user group, their needs and tasks, we propose a human-centered approach for facilitating game designers to co-create with AI/ML techniques through XAID. We illustrate our initial XAID framework through three use cases, which require an understanding both of the innate properties of the {AI} techniques and users' needs, and we identify key open challenges.},
   doi = {10.1109/CIG.2018.8490433}
}


@article{zednik2019solving,
  title={Solving the black box problem: a normative framework for explainable artificial intelligence},
  author={Zednik, Carlos},
  journal={Philosophy \& Technology},
  pages={1--24},
  year={2019},
  publisher={Springer}
}

@article{zhang2019pathologist,
  title={Pathologist-level interpretable whole-slide cancer diagnosis with deep learning},
  author={Zhang, Zizhao and Chen, Pingjun and McGough, Mason and Xing, Fuyong and Wang, Chunbao and Bui, Marilyn and Xie, Yuanpu and Sapkota, Manish and Cui, Lei and Dhillon, Jasreman and others},
  journal={Nature Machine Intelligence},
  volume={1},
  number={5},
  pages={236--245},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{zhou_comprehensive_2020,
	title = {A {Comprehensive} {Review} for {Breast} {Histopathology} {Image} {Analysis} {Using} {Classical} and {Deep} {Neural} {Networks}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2993788},
	abstract = {Breast cancer is one of the most common and deadliest cancers among women. Since histopathological images contain sufficient phenotypic information, they play an indispensable role in the diagnosis and treatment of breast cancers. To improve the accuracy and objectivity of Breast Histopathological Image Analysis (BHIA), Artificial Neural Network (ANN) approaches are widely used in the segmentation and classification tasks of breast histopathological images. In this review, we present a comprehensive overview of the BHIA techniques based on ANNs. First of all, we categorize the BHIA systems into classical and deep neural networks for in-depth investigation. Then, the relevant studies based on BHIA systems are presented. After that, we analyze the existing models to discover the most suitable algorithms. Finally, publicly accessible datasets, along with their download links, are provided for the convenience of future researchers.},
	journal = {IEEE Access},
	author = {Zhou, X. and Li, C. and Rahaman, M. M. and Yao, Y. and Ai, S. and Sun, C. and Wang, Q. and Zhang, Y. and Li, M. and Li, X. and Jiang, T. and Xue, D. and Qi, S. and Teng, Y.},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Machine learning, deep learning, Breast cancer, convolutional neural networks, Image analysis, Neural networks, image segmentation, histopathology, image classification},
	pages = {90931--90956},
	file = {IEEE Xplore Abstract Record:/Users/theoevans/Zotero/storage/QAV3DP67/9091012.html:text/html;IEEE Xplore Full Text PDF:/Users/theoevans/Zotero/storage/3V5YFV4R/Zhou et al. - 2020 - A Comprehensive Review for Breast Histopathology I.pdf:application/pdf}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% to regulatory landscape
% 21 citations - choice of norms and standards, also to be delivered and under development

@techreport{ DIN_DKE_Roadmap,
  title={ DIN DKE German Standardization Roadmap Artificial intelligence },
  author={ {DIN Deutsches Institut für Normung e. V.}},
  year={2020},
  institution={ DIN Deutsches Institut für Normung e. V. }
}

@techreport{DIN_ISO_31000,
  title={ DIN ISO 31000:2018-10 Risikomanagement - Leitlinien},
  author={ {International Organization for Standardization}},
  year={2018},
  institution={ International Organization for Standardization }
}

@techreport{DIN_SPEC_13266,
  title={ DIN SPEC 13266:2020-04 Leitfaden für die Entwicklung von Deep-Learning-Bilderkennungssystemen},
  author={{DIN Deutsches Institut für Normung e. V. }},
  year={2020},
  institution={ DIN Deutsches Institut für Normung e. V. }
}

%https://www.overleaf.com/project/60c7097a040adc813f3b4c1a

@techreport{ DIN_SPEC_13288,
  title={ DIN SPEC 13288:2021-03 Leitfaden für die Entwicklung von Deep-Learning-Bilderkennungssystemen in der Medizin },
  author={ DIN Deutsches Institut für Normung e. V. },
  year={2021},
  institution={ DIN Deutsches Institut für Normung e. V. }
}

@techreport{ DIN_SPEC_92001-1,
  title={ DIN SPEC 92001-1:2019-04 Künstliche Intelligenz - Life Cycle Prozesse und Qualitätsanforderungen –Teil 1: Qualitäts-Meta-Modell },
  author={ {DIN Deutsches Institut für Normung e. V. }},
  year={2019},
  institution={ DIN Deutsches Institut für Normung e. V.  }
}

@techreport{ DIN_SPEC_92001-2,
  title={ DIN SPEC 92001-1:2019-04 Künstliche Intelligenz - Life Cycle Prozesse und Qualitätsanforderungen –Teil 1: Qualitäts-Meta-Modell},
  author={ {DIN Deutsches Institut für Normung e. V. }},
  year={ 2019},
  institution={ DIN Deutsches Institut für Normung e. V.   }
}

@techreport{ ISO_IEC_TR_29119-11,
  title={ Technical Report (TR) ISO/IEC TR 29119-11:2020 Software and systems engineering – Software testing – Part 11: Guidelines on the testing of AI-based Systems },
  author={ {International Organization for Standardization}},
  year={2020},
  institution={ International Organization for Standardization }
}

@techreport{ ISO_IEC_25010,
  title={ ISO/IEC 25010 product quality model - Usability/Reliability },
  author={ {International Organization for Standardization}},
  year={2011},
  institution={ International Organization for Standardization }
}

@techreport{ ISO_IEC_TR_24029-1,
  title={ ISO/IEC TR 24029-1:2021 Artificial Intelligence ({AI})-Assessment of the robustness of neural networks - Part 1: Overview },
  author={ {International Organization for Standardization} },
  year={2021},
  institution={ International Organization for Standardization}
}

@techreport{ ISO_IEC_TR_24028,
  title={ ISO/IEC TR 24028:2020 Information technology - Artificial intelligence - Overview of trustworthiness in artificial intelligence
 },
  author={ {International Organization for Standardization}},
  year={2020},
  institution={ International Organization for Standardization}
}

@techreport{ISO_IEC_22989,
  title={ ISO/IEC 22989, Artificial intelligence – Concepts and terminology.},
  author={{International Organization for Standardization }},
  year={TBD},
  institution={ International Organization for Standardization }
}

@techreport{ ISO_IEC_23053,
  title={ ISO/IEC 23053, Framework for Artificial Intelligence (AI) Systems Using Machine Learning (ML) },
  author={{International Organization for Standardization }},
  year={TBD},
  institution={ International Organization for Standardization }
}

@techreport{ ISO_IEC_23894,
  title={ ISO/IEC 23894, Information Technology – Artificial Intelligence – Risk Management },
  author={{International Organization for Standardization }},
  year={TBD},
  institution={ International Organization for Standardization }
}

@techreport{ ISO_IEC_38507,
  title={ ISO/IEC 38507, Information technology – Governance of IT },
  author={{International Organization for Standardization }},
  year={TBD},
  institution={ International Organization for Standardization }
}

@techreport{ ISO_IEC_20546,
  title={ ISO/IEC 20546, Information technology – Big data },
  author={{International Organization for Standardization }},
  year={TBD},
  institution={ International Organization for Standardization }
}

@techreport{ ISO_IEC_5059,
  title={ → ISO/IEC 5059, Software engineering – Systems and software Quality Requirements and Evaluation (SQuaRE) },
  author={ {International Organization for Standardization}},
  year={TBD},
  institution={ International Organization for Standardization }
}

@techreport{ISO_IEC_TR_24027,
  title={ISO/IEC TR 24027, Information technology – Artificial Intelligence ({AI} )– Bias in {AI} systems and {AI} aided decision making },
  author={ {International Organization for Standardization}},
  year={TBD},
  institution={ International Organization for Standardization }
}

@techreport{ISO_IEC_TR_24368,
  title={ ISO/IEC TR 24368, Information technology – Artificial intelligence – Overview of ethical and societal concerns },
  author={{International Organization for Standardization} },
  year={TBD},
  institution={ International Organization for Standardization }
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{EU_White,
  title={ White Paper on Artificial Intelligence: a {European} approach to excellence and trust},
  author={ {European Commission of EU}},
  year={2020},
  institution={ European Commission of EU}
}

@misc{ GDPR,
  title={ },
  author={ },
  year={2021},
  publisher={ European Commission}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{Algo.rules,
  title={Praxisleitfaden zu den Algo.Rules Orientierungshilfen für Entwickler:innen und ihre Führungskräfte },
  author={ Michael Puntschuh, Lajla Fetic },
  year={2020},
  publisher={ Bertelsmann Stiftung},
 doi ={ DOI 10.11586/2020029 }
}

@misc{devsoft-baltic-ou-2021,
	author = {{Devsoft Baltic OÜ}},
	title = {{SurveyJS: Free Online Survey and Quiz Tools}},
	url = {https://surveyjs.io/},
	year = {2021},
}

@misc{empaia-website,
	author = {Hufnagl, P},
	title = {{EMPAIA Website}},
	url = {www.empaia.org},
	year = {2021},
}

@misc{evans-2021,
	author = {Evans, T and Retzlaff, C},
	title = {{xai-in-digital-pathology}},
	url = {https://github.com/theodore-evans/xai-in-digital-pathology},
	year = {2021 [Online]},
}

@misc{diffmorph:github,
	author = {Alexey Borsky},
	title = {{DiffMorph}},
	url = {https://github.com/volotat/DiffMorph},
	year = {2021 [Online]},
}

@techreport{daicom:supplement:222,
	title = {{Digital Imaging and Communications in Medicine (DICOM) - Supplement 222: Microscopy Bulk Simple Annotations Storage SOP Class}},
	author = {DICOM Standards Committee, Working Group 26 (Pathology)},
	institution = {DICOM Standards Committee},
	url = {https://www.dicomstandard.org/news-dir/current/docs/sups/sup222.pdf},
	year = {2021}
}

% @misc{noauthor_pathonet_2021,
% 	title = {{PathoNet} introduced as a deep neural network backend for evaluation of {Ki}-67 and tumor-infiltrating lymphocytes in breast cancer},
% 	copyright = {MIT},
% 	url = {https://github.com/SHIDCenter/PathoNet},
% 	urldate = {2021-08-01},
% 	publisher = {Shiraz Histopathological Imaging Dataset Center},
% 	month = jul,
% 	year = {2021},
% 	note = {original-date: 2020-06-09T14:22:03Z},
% }

@misc{otterai-2021,
	author = {{Otter.ai}"},
	title = {{Otter Voice Meeting Notes}},
	url = {https://otter.ai/},
	year = {2021},
}

@misc{business-wire-2021,
	author = {{Business Wire}},
	month = {05},
	title = {{Mindpeak Announces CE-IVD Mark for Artificial Intelligence Breast Cancer Cell Quantification}},
	url = {https://www.businesswire.com/news/home/20210519005046/en/Mindpeak-Announces-CE-IVD-Mark-for-Artificial-Intelligence-Breast-Cancer-Cell-Quantification},
	year = {2021},
}


@article{sendak2020presenting,
  title={Presenting machine learning model information to clinical end users with model facts labels},
  author={Sendak, Mark P and Gao, Michael and Brajer, Nathan and Balu, Suresh},
  journal={NPJ digital medicine},
  volume={3},
  number={1},
  pages={1--4},
  year={2020},
  publisher={Nature Publishing Group}
}

@misc{foote2021now,
      title={{Now You See It, Now You Don't}: Adversarial Vulnerabilities in Computational Pathology}, 
      author={Alex Foote and Amina Asif and Ayesha Azam and Tim Marshall-Cox and Nasir Rajpoot and Fayyaz Minhas},
      year={2021},
      eprint={2106.08153},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@article{yang2021unbox,
  title={Unbox the black-box for the medical explainable {AI} via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond},
  author={Yang, Guang and Ye, Qinghao and Xia, Jun},
  journal={arXiv preprint arXiv:2102.01998},
  year={2021}
}

% % Duplicate of pocevivciute_2020_survey
% @incollection{pocevivciute2020survey,
%   title={Survey of {xAI} in digital pathology},
%   author={Pocevi{\v{c}}i{\=u}t{\.e}, Milda and Eilertsen, Gabriel and Lundstr{\"o}m, Claes},
%   booktitle={Artificial Intelligence and Machine Learning for Digital Pathology},
%   pages={56--88},
%   year={2020},
%   publisher={Springer}
% }

@article{deshpande2021brief,
  title={A Brief Bibliometric Survey of Explainable {AI} in Medical Field},
  author={Deshpande, Nilkanth Mukund and Gite, Shilpa Shailesh},
  journal={Library Philosophy and Practice},
  pages={1--27},
  year={2021},
  publisher={Library Philosophy and Practice}
}

@article{jaume2020towards,
  title={Towards explainable graph representations in digital pathology},
  author={Jaume, Guillaume and Pati, Pushpak and Foncubierta-Rodriguez, Antonio and Feroce, Florinda and Scognamiglio, Giosue and Anniciello, Anna Maria and Thiran, Jean-Philippe and Goksel, Orcun and Gabrani, Maria},
  journal={arXiv preprint arXiv:2007.00311},
  year={2020}
}

@inproceedings{wang2021ai,
  title={{AI Trust Score}: A User-Centered Approach to Building, Designing, and Measuring the Success of Intelligent Workplace Features},
  author={Wang, Jennifer and Moulden, Angela},
  booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2021}
}

@article{lin2019explanations,
  title={Do explanations reflect decisions? A machine-centric strategy to quantify the performance of explainability algorithms},
  author={Lin, Zhong Qiu and Shafiee, Mohammad Javad and Bochkarev, Stanislav and Jules, Michael St and Wang, Xiao Yu and Wong, Alexander},
  journal={arXiv preprint arXiv:1910.07387},
  year={2019}
}

@inproceedings{jiang2018trust,
  title={To trust or not to trust a classifier},
  author={Jiang, Heinrich and Kim, Been and Guan, Melody Y and Gupta, Maya},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={5546--5557},
  year={2018}
}

@article{springenberg2014striving,
  title={Striving for simplicity: The all convolutional net},
  author={Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  journal={ICLR Workshop},
  year={2014}
}

@article{seah2019chest,
  title={Chest radiographs in congestive heart failure: visualizing neural network learning},
  author={Seah, Jarrel CY and Tang, Jennifer SN and Kitchen, Andy and Gaillard, Frank and Dixon, Andrew F},
  journal={Radiology},
  volume={290},
  number={2},
  pages={514--522},
  year={2019},
  publisher={Radiological Society of North America}
}

@article{gulshad2021counterfactual,
  title={Counterfactual attribute-based visual explanations for classification},
  author={Gulshad, Sadaf and Smeulders, Arnold},
  journal={International Journal of Multimedia Information Retrieval},
  volume={10},
  number={2},
  pages={127--140},
  year={2021},
  publisher={Springer}
}

@inproceedings{pearce2018high,
  title={High-quality prediction intervals for deep learning: A distribution-free, ensembled approach},
  author={Pearce, Tim and Brintrup, Alexandra and Zaki, Mohamed and Neely, Andy},
  booktitle={International Conference on Machine Learning},
  pages={4075--4084},
  year={2018},
  organization={PMLR}
}

@article{tagasovska2019single,
  title={Single-model uncertainties for deep learning},
  author={Tagasovska, Natasa and Lopez-Paz, David},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={6417--6428},
  year={2019}
}

@article{kim2016examples,
  title={Examples are not enough, learn to criticize! criticism for interpretability},
  author={Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{li2018deep,
  title={Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions},
  author={Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  issue={1},
  year={2018}
}

@inproceedings{abdul2020cogam,
  title={COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations},
  author={Abdul, Ashraf and von der Weth, Christian and Kankanhalli, Mohan and Lim, Brian Y},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--14},
  year={2020}
}

@inproceedings{liu2019generative,
  title={Generative counterfactual introspection for explainable deep learning},
  author={Liu, Shusen and Kailkhura, Bhavya and Loveland, Donald and Han, Yong},
  booktitle={2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP)},
  pages={1--5},
  year={2019},
  organization={IEEE}
}

@article{yosinski2015deepvisualization,
  title={Understanding neural networks through deep visualization},
  author={Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  journal={arXiv e-prints},
  pages={arXiv--1506},
  year={2015}
}

@inproceedings{ribeiro2016trust,
  title={"{Why should I trust you?}" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@article{erhan2009visualizing,
  title={Visualizing higher-layer features of a deep network},
  author={Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={University of Montreal},
  volume={1341},
  number={3},
  pages={1},
  year={2009}
}

@article{zhang2021survey,
  title={A survey on neural network interpretability},
  author={Zhang, Yu and Ti{\v{n}}o, Peter and Leonardis, Ale{\v{s}} and Tang, Ke},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  year={2021},
  publisher={IEEE}
}

@article{ginsberg1986counterfactuals,
  title={Counterfactuals},
  author={Ginsberg, Matthew L},
  journal={Artificial intelligence},
  volume={30},
  number={1},
  pages={35--79},
  year={1986},
  publisher={Elsevier}
}

@misc{EU:2021:52021PC0206,
  author={{Council of European Union}},
  title={{Proposal for a Regulation of the European Parliament and the Council laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union legislative acts.}},
  year={2021},
  note={\newline\url{https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELLAR:e0649735-a372-11eb-9585-01aa75ed71a1}
 }
}



@article{li2015ki67,
  title={Ki67 is a promising molecular target in the diagnosis of cancer},
  author={Li, Lian Tao and Jiang, Guan and Chen, Qian and Zheng, Jun Nian},
  journal={Molecular medicine reports},
  volume={11},
  number={3},
  pages={1566--1572},
  year={2015},
  publisher={Spandidos Publications}
}

@article{scholzen2000ki,
  title={The Ki-67 protein: from the known and the unknown},
  author={Scholzen, Thomas and Gerdes, Johannes},
  journal={Journal of cellular physiology},
  volume={182},
  number={3},
  pages={311--322},
  year={2000},
  publisher={Wiley Online Library}
}

@inproceedings{ribeiro2018anchors,
  title={Anchors: High-precision model-agnostic explanations},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  year={2018}
}

@article{doshi2017towards,
  title={Towards a rigorous science of interpretable machine learning},
  author={Doshi-Velez, Finale and Kim, Been},
  journal={arXiv preprint arXiv:1702.08608},
  year={2017}
}

@article{miller2019explanation,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={Artificial intelligence},
  volume={267},
  pages={1--38},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{wang2019designing,
  title={Designing theory-driven user-centric explainable AI},
  author={Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y},
  booktitle={Proceedings of the 2019 CHI conference on human factors in computing systems},
  pages={1--15},
  year={2019}
}

@inproceedings{cai2019effects,
  title={The effects of example-based explanations in a machine learning interface},
  author={Cai, Carrie J and Jongejan, Jonas and Holbrook, Jess},
  booktitle={Proceedings of the 24th international conference on intelligent user interfaces},
  pages={258--262},
  year={2019}
}

@article{lipton2018mythos,
  title={The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery.},
  author={Lipton, Zachary C},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{hemmer2021human,
  title={Human-ai complementarity in hybrid intelligence systems: A structured literature review},
  author={Hemmer, Patrick and Schemmer, Max and V{\"o}ssing, Michael and K{\"u}hl, Niklas},
  journal={PACIS 2021 Proceedings},
  year={2021}
}

@article{feldman2016simplicity,
  title={The simplicity principle in perception and cognition},
  author={Feldman, Jacob},
  journal={Wiley Interdisciplinary Reviews: Cognitive Science},
  volume={7},
  number={5},
  pages={330--340},
  year={2016},
  publisher={Wiley Online Library}
}

@article{jussupow2021augmenting,
  title={Augmenting medical diagnosis decisions? An investigation into physicians’ decision-making process with artificial intelligence},
  author={Jussupow, Ekaterina and Spohrer, Kai and Heinzl, Armin and Gawlitza, Joshua},
  journal={Information Systems Research},
  year={2021},
  publisher={INFORMS}
}

@inproceedings{kastner2021relation,
  title={On the Relation of Trust and Explainability: Why to Engineer for Trustworthiness},
  author={K{\"a}stner, Lena and Langer, Markus and Lazar, Veronika and Schom{\"a}cker, Astrid and Speith, Timo and Sterz, Sarah},
  booktitle={2021 IEEE 29th International Requirements Engineering Conference Workshops (REW)},
  pages={169--175},
  year={2021},
  organization={IEEE}
}

@inproceedings{de2017people,
  title={How people explain action (and autonomous intelligent systems should too)},
  author={De Graaf, Maartje MA and Malle, Bertram F},
  booktitle={2017 AAAI Fall Symposium Series},
  year={2017}
}

@article{hoffman2017explainingpart1,
  title={Explaining explanation, part 1: theoretical foundations},
  author={Hoffman, Robert R and Klein, Gary},
  journal={IEEE Intelligent Systems},
  volume={32},
  number={3},
  pages={68--73},
  year={2017},
  publisher={IEEE}
}

@article{hoffman2017explainingpart2,
  title={Explaining explanation, part 2: Empirical foundations},
  author={Hoffman, Robert R and Mueller, Shane T and Klein, Gary},
  journal={IEEE Intelligent Systems},
  volume={32},
  number={4},
  pages={78--86},
  year={2017},
  publisher={IEEE}
}

@article{klein2018explainingpart3,
  title={Explaining explanation, part 3: The causal landscape},
  author={Klein, Gary},
  journal={IEEE Intelligent Systems},
  volume={33},
  number={2},
  pages={83--88},
  year={2018},
  publisher={IEEE}
}

@article{hoffman2018explainingpart4,
  title={Explaining explanation, part 4: a deep dive on deep nets},
  author={Hoffman, Robert and Miller, Tim and Mueller, Shane T and Klein, Gary and Clancey, William J},
  journal={IEEE Intelligent Systems},
  volume={33},
  number={3},
  pages={87--95},
  year={2018},
  publisher={IEEE}
}

@book{crichton1991jurassic,
  title={Jurassic Park : [novel]},
  author={Crichton, M.},
  isbn={9780345370778},
  lccn={91023582},
  series={Ballantine Books},
  url={https://books.google.se/books?id=I79dV\_HQvykC},
  year={1991},
  publisher={Ballantine Books}
}