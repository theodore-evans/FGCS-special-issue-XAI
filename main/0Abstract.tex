The increasing prevalence of digitized workflows opens the door to life-saving applications of artificial intelligence in diagnostic pathology. The need for explainability in such systems is key to their approval and acceptance for clinical use. Despite the inherent cross-disciplinary challenge of building explainable AI (xAI) systems for digital pathology, very few application- and user- centric studies in this domain have been conducted. We conducted the first mixed-methods study evaluating user interpretations of the state of the art in explanation-generating methods, as applied to a common use case in AI-assisted digital pathology. This paper reveals valuable insights to help machine learning researchers, AI developers and human computer interface (HCI) designers build safer and more effective xAI solutions.