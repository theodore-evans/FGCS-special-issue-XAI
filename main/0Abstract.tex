The increasing prevalence of digitized workflows opens the door to life-saving applications of artificial intelligence (AI) in diagnostic pathology. Explainability is identified as a critical component for the safety, approval and acceptance of such systems for clinical use. Despite the cross-disciplinary challenge of building explainable AI (xAI), very few application- and user-centric studies in this domain have been conducted. We conducted the first mixed-methods study evaluating user interpretations of examples from the state of the art in explanation-generating methods, in the context of a common use case of AI-assisted digital pathology. This paper reveals valuable insights into the challenging dillemas that machine learning researchers, AI developers and human-computer interface (HCI) designers must overcome together in order to build safer and more effective xAI solutions.