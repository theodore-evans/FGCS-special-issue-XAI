The increasing prevalence of digitized workflows opens the door to life-saving applications of artificial intelligence (AI) in diagnostic pathology. Explainability is identified as a critical component for the safety, approval and acceptance of such systems for clinical use. Despite the cross-disciplinary challenge of building explainable AI (xAI), very few application- and user-centric studies in this domain have been carried out. We conduct the first mixed-methods study of user interaction with samples from the state-of-the-art in explainability for digital pathology. This study reveals challenging dilemmas faced by developers of xAI solutions for medicine, as well as empirically-backed principles for their safer and more effective design.